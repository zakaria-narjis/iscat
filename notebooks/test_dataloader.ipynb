{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Directory: d:\\workspace\\iscat\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.chdir(\"..\")\n",
    "print(\"Current Directory:\", os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.data_processing.dataset import iScatDataset\n",
    "from src.data_processing.utils import Utils\n",
    "data_path = os.path.join('data', 'iScat', 'Data', '2024_11_11', 'Metasurface', 'Chip_02')\n",
    "image_paths,target_paths = Utils.get_data_paths(data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading surface images to Memory: 100%|██████████| 4/4 [00:07<00:00,  1.89s/it]\n",
      "Creating Masks: 100%|██████████| 4/4 [00:00<00:00, 364.62it/s]\n",
      "Loading surface images to Memory: 100%|██████████| 1/1 [00:01<00:00,  1.90s/it]\n",
      "Creating Masks: 100%|██████████| 1/1 [00:00<00:00, 334.23it/s]\n"
     ]
    }
   ],
   "source": [
    "train_dataset = iScatDataset(image_paths[:-1], target_paths[:-1], preload_image=True)\n",
    "valid_dataset = iScatDataset([image_paths[-1]],[target_paths[-1]],preload_image=True,apply_augmentation=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, Dataset\n",
    "def create_dataloaders(train_dataset, test_dataset, batch_size=4):\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "    return train_loader, val_loader\n",
    "train_loader, val_loader = create_dataloaders(train_dataset, valid_dataset, batch_size=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "from sam2.build_sam import build_sam2\n",
    "from sam2.sam2_image_predictor import SAM2ImagePredictor\n",
    "import torchmetrics\n",
    "import tqdm\n",
    "\n",
    "class MulticlassSegmentationDataset(Dataset):\n",
    "    def __init__(self, images, masks):\n",
    "        \"\"\"\n",
    "        Initialize dataset with images and masks\n",
    "        \n",
    "        Args:\n",
    "            images (torch.Tensor): Tensor of images [N, C, H, W]\n",
    "            masks (torch.Tensor): Tensor of masks [N, C, H, W]\n",
    "        \"\"\"\n",
    "        self.images = images\n",
    "        self.masks = masks\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.images[idx], self.masks[idx]\n",
    "\n",
    "def create_dataloaders(train_images, train_masks, val_images, val_masks, batch_size=4):\n",
    "    \"\"\"\n",
    "    Create train and validation dataloaders\n",
    "    \"\"\"\n",
    "    train_dataset = MulticlassSegmentationDataset(train_images, train_masks)\n",
    "    val_dataset = MulticlassSegmentationDataset(val_images, val_masks)\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "    \n",
    "    return train_loader, val_loader\n",
    "\n",
    "class SAM2MulticlassTrainer:\n",
    "    def __init__(self, model_cfg, checkpoint, num_classes, device='cuda'):\n",
    "        \"\"\"\n",
    "        Initialize SAM2 trainer for multiclass segmentation\n",
    "        \n",
    "        Args:\n",
    "            model_cfg (str): Path to model configuration\n",
    "            checkpoint (str): Path to pre-trained checkpoint\n",
    "            num_classes (int): Number of segmentation classes\n",
    "            device (str): Training device\n",
    "        \"\"\"\n",
    "        self.device = device\n",
    "        self.num_classes = num_classes\n",
    "        \n",
    "        # Build SAM2 model\n",
    "        self.sam2_model = build_sam2(model_cfg, checkpoint, device=device)\n",
    "        self.predictor = SAM2ImagePredictor(self.sam2_model)\n",
    "        \n",
    "        # IoU and Dice metrics\n",
    "        self.iou_metric = torchmetrics.JaccardIndex(task='multiclass', num_classes=num_classes).to(device)\n",
    "        self.dice_metric = torchmetrics.Dice(num_classes=num_classes).to(device)\n",
    "        \n",
    "    def _prepare_box_prompt(self, image_size):\n",
    "        \"\"\"\n",
    "        Prepare a bounding box prompt covering the entire image\n",
    "        \n",
    "        Args:\n",
    "            image_size (tuple): Height and width of the image\n",
    "        \n",
    "        Returns:\n",
    "            torch.Tensor: Bounding box coordinates\n",
    "        \"\"\"\n",
    "        h, w = image_size\n",
    "        return torch.tensor([[0, 0, w, h]], dtype=torch.float32).to(self.device)\n",
    "    \n",
    "    def train_model(self, train_loader, val_loader, epochs=50, patience=5):\n",
    "        \"\"\"\n",
    "        Train SAM2 model with early stopping\n",
    "        \n",
    "        First phase: Only train mask decoder (encoder frozen)\n",
    "        Second phase: Train entire model\n",
    "        \"\"\"\n",
    "        # Phase 1: Train only mask decoder\n",
    "        print(\"Phase 1: Training Mask Decoder\")\n",
    "        self._train_phase(train_loader, val_loader, epochs, patience, freeze_encoder=True)\n",
    "        \n",
    "        # Phase 2: Train entire model\n",
    "        print(\"Phase 2: Training Entire Model\")\n",
    "        self._train_phase(train_loader, val_loader, epochs, patience, freeze_encoder=False)\n",
    "    \n",
    "    def _train_phase(self, train_loader, val_loader, epochs, patience, freeze_encoder=True):\n",
    "        \"\"\"\n",
    "        Perform training phase with early stopping\n",
    "        \"\"\"\n",
    "        # Freeze/unfreeze encoder based on phase\n",
    "        for param in self.predictor.model.image_encoder.parameters():\n",
    "            param.requires_grad = not freeze_encoder\n",
    "        \n",
    "        # Only train decoder and prompt encoder\n",
    "        trainable_params = list(self.predictor.model.sam_mask_decoder.parameters()) + \\\n",
    "                           list(self.predictor.model.sam_prompt_encoder.parameters())\n",
    "        \n",
    "        optimizer = torch.optim.AdamW(trainable_params, lr=1e-4, weight_decay=1e-4)\n",
    "        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', patience=2)\n",
    "        \n",
    "        best_val_iou = 0\n",
    "        patience_counter = 0\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            self.predictor.model.train()\n",
    "            train_losses, train_ious = [], []\n",
    "            \n",
    "            for images, masks in tqdm.tqdm(train_loader, desc=f\"Epoch {epoch+1}\"):\n",
    "                images, masks = images.to(self.device), masks.to(self.device)\n",
    "                optimizer.zero_grad()\n",
    "                \n",
    "                # Prepare batch results\n",
    "                batch_losses, batch_ious = [], []\n",
    "                \n",
    "                for img, gt_mask in zip(images, masks):\n",
    "                    self.predictor.set_image(img.numpy())\n",
    "                    \n",
    "                    # Whole image box prompt\n",
    "                    box_input = self._prepare_box_prompt(img.shape[1:])\n",
    "                    \n",
    "                    # Prompt encoding\n",
    "                    sparse_embeddings, dense_embeddings = self.predictor.model.sam_prompt_encoder(\n",
    "                        points=None, boxes=box_input, masks=None\n",
    "                    )\n",
    "                    \n",
    "                    # Predict masks\n",
    "                    low_res_masks, _, _, _ = self.predictor.model.sam_mask_decoder(\n",
    "                        image_embeddings=self.predictor._features[\"image_embed\"][-1].unsqueeze(0),\n",
    "                        image_pe=self.predictor.model.sam_prompt_encoder.get_dense_pe(),\n",
    "                        sparse_prompt_embeddings=sparse_embeddings,\n",
    "                        dense_prompt_embeddings=dense_embeddings,\n",
    "                        multimask_output=False\n",
    "                    )\n",
    "                    \n",
    "                    # Postprocess masks\n",
    "                    prd_masks = self.predictor._transforms.postprocess_masks(\n",
    "                        low_res_masks, self.predictor._orig_hw[-1]\n",
    "                    )\n",
    "                    \n",
    "                    # Sigmoid and multi-class handling\n",
    "                    prd_masks = torch.sigmoid(prd_masks)\n",
    "                    prd_masks_multi = prd_masks.repeat(self.num_classes, 1, 1, 1)\n",
    "                    \n",
    "                    # Multi-class segmentation loss\n",
    "                    seg_loss = nn.functional.cross_entropy(\n",
    "                        prd_masks_multi,  # predicted masks\n",
    "                        torch.argmax(gt_mask, dim=0),  # convert one-hot to class indices\n",
    "                        reduction='mean'\n",
    "                    )\n",
    "                    \n",
    "                    # IoU calculation\n",
    "                    prd_class_masks = (prd_masks_multi > 0.5).float()\n",
    "                    iou = self.iou_metric(prd_class_masks, gt_mask)\n",
    "                    \n",
    "                    batch_losses.append(seg_loss)\n",
    "                    batch_ious.append(iou)\n",
    "                \n",
    "                # Aggregate batch results\n",
    "                loss = torch.mean(torch.stack(batch_losses))\n",
    "                iou = torch.mean(torch.stack(batch_ious))\n",
    "                \n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                \n",
    "                train_losses.append(loss.item())\n",
    "                train_ious.append(iou.item())\n",
    "            \n",
    "            # Validation\n",
    "            self.predictor.model.eval()\n",
    "            val_losses, val_ious = [], []\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                for images, masks in val_loader:\n",
    "                    images, masks = images.to(self.device), masks.to(self.device)\n",
    "                    \n",
    "                    batch_val_losses, batch_val_ious = [], []\n",
    "                    \n",
    "                    for img, gt_mask in zip(images, masks):\n",
    "                        # Similar prediction logic as training\n",
    "                        # (omitted for brevity, would mirror training code)\n",
    "                        pass\n",
    "                    \n",
    "                    # Aggregate validation metrics\n",
    "            \n",
    "            # Update learning rate and check early stopping\n",
    "            avg_val_iou = np.mean(val_ious)\n",
    "            scheduler.step(avg_val_iou)\n",
    "            \n",
    "            if avg_val_iou > best_val_iou:\n",
    "                best_val_iou = avg_val_iou\n",
    "                patience_counter = 0\n",
    "                torch.save(self.predictor.model.state_dict(), f'best_model_{\"encoder_frozen\" if freeze_encoder else \"full_model\"}.pth')\n",
    "            else:\n",
    "                patience_counter += 1\n",
    "            \n",
    "            if patience_counter >= patience:\n",
    "                print(f\"Early stopping at epoch {epoch+1}\")\n",
    "                break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = \"src\\model\\sam2\\checkpoints\\sam2.1_hiera_small.pt\"\n",
    "model_cfg = \"configs/sam2.1/sam2.1_hiera_s.yaml\"\n",
    "sam2_model = build_sam2(model_cfg, checkpoint,device='cuda')\n",
    "predictor = SAM2ImagePredictor(sam2_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ImageEncoder(\n",
       "  (trunk): Hiera(\n",
       "    (patch_embed): PatchEmbed(\n",
       "      (proj): Conv2d(3, 96, kernel_size=(7, 7), stride=(4, 4), padding=(3, 3))\n",
       "    )\n",
       "    (blocks): ModuleList(\n",
       "      (0): MultiScaleBlock(\n",
       "        (norm1): LayerNorm((96,), eps=1e-06, elementwise_affine=True)\n",
       "        (attn): MultiScaleAttention(\n",
       "          (qkv): Linear(in_features=96, out_features=288, bias=True)\n",
       "          (proj): Linear(in_features=96, out_features=96, bias=True)\n",
       "        )\n",
       "        (drop_path): Identity()\n",
       "        (norm2): LayerNorm((96,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (layers): ModuleList(\n",
       "            (0): Linear(in_features=96, out_features=384, bias=True)\n",
       "            (1): Linear(in_features=384, out_features=96, bias=True)\n",
       "          )\n",
       "          (act): GELU(approximate='none')\n",
       "        )\n",
       "      )\n",
       "      (1): MultiScaleBlock(\n",
       "        (norm1): LayerNorm((96,), eps=1e-06, elementwise_affine=True)\n",
       "        (pool): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
       "        (attn): MultiScaleAttention(\n",
       "          (q_pool): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
       "          (qkv): Linear(in_features=96, out_features=576, bias=True)\n",
       "          (proj): Linear(in_features=192, out_features=192, bias=True)\n",
       "        )\n",
       "        (drop_path): Identity()\n",
       "        (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (layers): ModuleList(\n",
       "            (0): Linear(in_features=192, out_features=768, bias=True)\n",
       "            (1): Linear(in_features=768, out_features=192, bias=True)\n",
       "          )\n",
       "          (act): GELU(approximate='none')\n",
       "        )\n",
       "        (proj): Linear(in_features=96, out_features=192, bias=True)\n",
       "      )\n",
       "      (2): MultiScaleBlock(\n",
       "        (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "        (attn): MultiScaleAttention(\n",
       "          (qkv): Linear(in_features=192, out_features=576, bias=True)\n",
       "          (proj): Linear(in_features=192, out_features=192, bias=True)\n",
       "        )\n",
       "        (drop_path): Identity()\n",
       "        (norm2): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (layers): ModuleList(\n",
       "            (0): Linear(in_features=192, out_features=768, bias=True)\n",
       "            (1): Linear(in_features=768, out_features=192, bias=True)\n",
       "          )\n",
       "          (act): GELU(approximate='none')\n",
       "        )\n",
       "      )\n",
       "      (3): MultiScaleBlock(\n",
       "        (norm1): LayerNorm((192,), eps=1e-06, elementwise_affine=True)\n",
       "        (pool): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
       "        (attn): MultiScaleAttention(\n",
       "          (q_pool): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
       "          (qkv): Linear(in_features=192, out_features=1152, bias=True)\n",
       "          (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "        )\n",
       "        (drop_path): Identity()\n",
       "        (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (layers): ModuleList(\n",
       "            (0): Linear(in_features=384, out_features=1536, bias=True)\n",
       "            (1): Linear(in_features=1536, out_features=384, bias=True)\n",
       "          )\n",
       "          (act): GELU(approximate='none')\n",
       "        )\n",
       "        (proj): Linear(in_features=192, out_features=384, bias=True)\n",
       "      )\n",
       "      (4-13): 10 x MultiScaleBlock(\n",
       "        (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "        (attn): MultiScaleAttention(\n",
       "          (qkv): Linear(in_features=384, out_features=1152, bias=True)\n",
       "          (proj): Linear(in_features=384, out_features=384, bias=True)\n",
       "        )\n",
       "        (drop_path): Identity()\n",
       "        (norm2): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (layers): ModuleList(\n",
       "            (0): Linear(in_features=384, out_features=1536, bias=True)\n",
       "            (1): Linear(in_features=1536, out_features=384, bias=True)\n",
       "          )\n",
       "          (act): GELU(approximate='none')\n",
       "        )\n",
       "      )\n",
       "      (14): MultiScaleBlock(\n",
       "        (norm1): LayerNorm((384,), eps=1e-06, elementwise_affine=True)\n",
       "        (pool): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
       "        (attn): MultiScaleAttention(\n",
       "          (q_pool): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
       "          (qkv): Linear(in_features=384, out_features=2304, bias=True)\n",
       "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (drop_path): Identity()\n",
       "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (layers): ModuleList(\n",
       "            (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (1): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (act): GELU(approximate='none')\n",
       "        )\n",
       "        (proj): Linear(in_features=384, out_features=768, bias=True)\n",
       "      )\n",
       "      (15): MultiScaleBlock(\n",
       "        (norm1): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (attn): MultiScaleAttention(\n",
       "          (qkv): Linear(in_features=768, out_features=2304, bias=True)\n",
       "          (proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "        )\n",
       "        (drop_path): Identity()\n",
       "        (norm2): LayerNorm((768,), eps=1e-06, elementwise_affine=True)\n",
       "        (mlp): MLP(\n",
       "          (layers): ModuleList(\n",
       "            (0): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (1): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          )\n",
       "          (act): GELU(approximate='none')\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (neck): FpnNeck(\n",
       "    (position_encoding): PositionEmbeddingSine()\n",
       "    (convs): ModuleList(\n",
       "      (0): Sequential(\n",
       "        (conv): Conv2d(768, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "      )\n",
       "      (1): Sequential(\n",
       "        (conv): Conv2d(384, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "      )\n",
       "      (2): Sequential(\n",
       "        (conv): Conv2d(192, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "      )\n",
       "      (3): Sequential(\n",
       "        (conv): Conv2d(96, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sam2_model.image_encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Phase 1: Training Mask Decoder\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:   0%|          | 0/100 [00:00<?, ?it/s]\n",
      "Exception ignored in: <function _MultiProcessingDataLoaderIter.__del__ at 0x000002A4C63E3B00>\n",
      "Traceback (most recent call last):\n",
      "  File \"d:\\anaconda3\\envs\\iscat\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py\", line 1604, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"d:\\anaconda3\\envs\\iscat\\Lib\\site-packages\\torch\\utils\\data\\dataloader.py\", line 1562, in _shutdown_workers\n",
      "    if self._persistent_workers or self._workers_status[worker_id]:\n",
      "                                   ^^^^^^^^^^^^^^^^^^^^\n",
      "AttributeError: '_MultiProcessingDataLoaderIter' object has no attribute '_workers_status'\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The following operation failed in the TorchScript interpreter.\nTraceback of TorchScript (most recent call last):\n  File \"d:\\anaconda3\\envs\\iscat\\Lib\\site-packages\\torch\\nn\\modules\\container.py\", line 250, in forward\n    def forward(self, input):\n        for module in self:\n            input = module(input)\n                    ~~~~~~ <--- HERE\n        return input\n  File \"d:\\anaconda3\\envs\\iscat\\Lib\\site-packages\\torchvision\\transforms\\transforms.py\", line 277, in forward\n            Tensor: Normalized Tensor image.\n        \"\"\"\n        return F.normalize(tensor, self.mean, self.std, self.inplace)\n               ~~~~~~~~~~~ <--- HERE\n  File \"d:\\anaconda3\\envs\\iscat\\Lib\\site-packages\\torchvision\\transforms\\functional.py\", line 350, in normalize\n        raise TypeError(f\"img should be Tensor Image. Got {type(tensor)}\")\n\n    return F_t.normalize(tensor, mean=mean, std=std, inplace=inplace)\n           ~~~~~~~~~~~~~ <--- HERE\n  File \"d:\\anaconda3\\envs\\iscat\\Lib\\site-packages\\torchvision\\transforms\\_functional_tensor.py\", line 928, in normalize\n    if std.ndim == 1:\n        std = std.view(-1, 1, 1)\n    return tensor.sub_(mean).div_(std)\n           ~~~~~~~~~~~ <--- HERE\nRuntimeError: The size of tensor a (224) must match the size of tensor b (3) at non-singleton dimension 0\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[30], line 7\u001b[0m\n\u001b[0;32m      1\u001b[0m trainer \u001b[38;5;241m=\u001b[39m SAM2MulticlassTrainer(\n\u001b[0;32m      2\u001b[0m     model_cfg\u001b[38;5;241m=\u001b[39mmodel_cfg , \n\u001b[0;32m      3\u001b[0m     checkpoint\u001b[38;5;241m=\u001b[39mcheckpoint , \n\u001b[0;32m      4\u001b[0m     num_classes\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m,\n\u001b[0;32m      5\u001b[0m     device\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m      6\u001b[0m )\n\u001b[1;32m----> 7\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[29], line 85\u001b[0m, in \u001b[0;36mSAM2MulticlassTrainer.train_model\u001b[1;34m(self, train_loader, val_loader, epochs, patience)\u001b[0m\n\u001b[0;32m     83\u001b[0m \u001b[38;5;66;03m# Phase 1: Train only mask decoder\u001b[39;00m\n\u001b[0;32m     84\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPhase 1: Training Mask Decoder\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 85\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_train_phase\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpatience\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfreeze_encoder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m     87\u001b[0m \u001b[38;5;66;03m# Phase 2: Train entire model\u001b[39;00m\n\u001b[0;32m     88\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPhase 2: Training Entire Model\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[29], line 121\u001b[0m, in \u001b[0;36mSAM2MulticlassTrainer._train_phase\u001b[1;34m(self, train_loader, val_loader, epochs, patience, freeze_encoder)\u001b[0m\n\u001b[0;32m    118\u001b[0m batch_losses, batch_ious \u001b[38;5;241m=\u001b[39m [], []\n\u001b[0;32m    120\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m img, gt_mask \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(images, masks):\n\u001b[1;32m--> 121\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredictor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mset_image\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnumpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    123\u001b[0m     \u001b[38;5;66;03m# Whole image box prompt\u001b[39;00m\n\u001b[0;32m    124\u001b[0m     box_input \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prepare_box_prompt(img\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m:])\n",
      "File \u001b[1;32md:\\anaconda3\\envs\\iscat\\Lib\\site-packages\\torch\\utils\\_contextlib.py:116\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[0;32m    114\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m    115\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[1;32m--> 116\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mD:\\workspace\\iscat\\src\\model\\sam2\\sam2\\sam2_image_predictor.py:110\u001b[0m, in \u001b[0;36mSAM2ImagePredictor.set_image\u001b[1;34m(self, image)\u001b[0m\n\u001b[0;32m    107\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    108\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mImage format not supported\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 110\u001b[0m input_image \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_transforms\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    111\u001b[0m input_image \u001b[38;5;241m=\u001b[39m input_image[\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m]\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m    113\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m (\n\u001b[0;32m    114\u001b[0m     \u001b[38;5;28mlen\u001b[39m(input_image\u001b[38;5;241m.\u001b[39mshape) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m4\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m input_image\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m3\u001b[39m\n\u001b[0;32m    115\u001b[0m ), \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_image must be of size 1x3xHxW, got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00minput_image\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[1;32mD:\\workspace\\iscat\\src\\model\\sam2\\sam2\\utils\\transforms.py:39\u001b[0m, in \u001b[0;36mSAM2Transforms.__call__\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     37\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m     38\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mto_tensor(x)\n\u001b[1;32m---> 39\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransforms\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\anaconda3\\envs\\iscat\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\anaconda3\\envs\\iscat\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[1;31mRuntimeError\u001b[0m: The following operation failed in the TorchScript interpreter.\nTraceback of TorchScript (most recent call last):\n  File \"d:\\anaconda3\\envs\\iscat\\Lib\\site-packages\\torch\\nn\\modules\\container.py\", line 250, in forward\n    def forward(self, input):\n        for module in self:\n            input = module(input)\n                    ~~~~~~ <--- HERE\n        return input\n  File \"d:\\anaconda3\\envs\\iscat\\Lib\\site-packages\\torchvision\\transforms\\transforms.py\", line 277, in forward\n            Tensor: Normalized Tensor image.\n        \"\"\"\n        return F.normalize(tensor, self.mean, self.std, self.inplace)\n               ~~~~~~~~~~~ <--- HERE\n  File \"d:\\anaconda3\\envs\\iscat\\Lib\\site-packages\\torchvision\\transforms\\functional.py\", line 350, in normalize\n        raise TypeError(f\"img should be Tensor Image. Got {type(tensor)}\")\n\n    return F_t.normalize(tensor, mean=mean, std=std, inplace=inplace)\n           ~~~~~~~~~~~~~ <--- HERE\n  File \"d:\\anaconda3\\envs\\iscat\\Lib\\site-packages\\torchvision\\transforms\\_functional_tensor.py\", line 928, in normalize\n    if std.ndim == 1:\n        std = std.view(-1, 1, 1)\n    return tensor.sub_(mean).div_(std)\n           ~~~~~~~~~~~ <--- HERE\nRuntimeError: The size of tensor a (224) must match the size of tensor b (3) at non-singleton dimension 0\n"
     ]
    }
   ],
   "source": [
    "trainer = SAM2MulticlassTrainer(\n",
    "    model_cfg=model_cfg , \n",
    "    checkpoint=checkpoint , \n",
    "    num_classes=3,\n",
    "    device='cpu'\n",
    ")\n",
    "trainer.train_model(train_loader, val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "iscat",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
