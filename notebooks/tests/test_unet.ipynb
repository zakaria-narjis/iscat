{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Directory: d:\\workspace\\iscat\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.chdir(\"..\")\n",
    "print(\"Current Directory:\", os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.data_processing.dataset import iScatDataset\n",
    "from src.data_processing.utils import Utils\n",
    "data_path = os.path.join('data', 'iScat', 'Data', '2024_11_11', 'Metasurface', 'Chip_02')\n",
    "image_paths,target_paths = Utils.get_data_paths(data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading surface images to Memory: 100%|██████████| 4/4 [00:44<00:00, 11.02s/it]\n",
      "Creating Masks: 100%|██████████| 4/4 [00:00<00:00, 73.58it/s]\n",
      "Loading surface images to Memory: 100%|██████████| 1/1 [00:14<00:00, 14.07s/it]\n",
      "Creating Masks: 100%|██████████| 1/1 [00:00<00:00, 77.13it/s]\n"
     ]
    }
   ],
   "source": [
    "train_dataset = iScatDataset(image_paths[:-1], target_paths[:-1], preload_image=True)\n",
    "valid_dataset = iScatDataset([image_paths[-1]],[target_paths[-1]],preload_image=True,apply_augmentation=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, Dataset\n",
    "def create_dataloaders(train_dataset, test_dataset, batch_size=4):\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "    return train_loader, val_loader\n",
    "train_loader, val_loader = create_dataloaders(train_dataset, valid_dataset, batch_size=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import numpy as np\n",
    "\n",
    "# Modify the model to support multiclass segmentation\n",
    "class MultiClassUNet(nn.Module):\n",
    "    def __init__(self, in_channels=3, num_classes=3, init_features=32):\n",
    "        super(MultiClassUNet, self).__init__()\n",
    "        \n",
    "        # Load the pretrained model and modify the final layer\n",
    "        model = torch.hub.load('mateuszbuda/brain-segmentation-pytorch', 'unet', \n",
    "                               in_channels=in_channels, \n",
    "                               out_channels=1, \n",
    "                               init_features=init_features, \n",
    "                               pretrained=True)\n",
    "        \n",
    "        # Replace the final convolution layer to match number of classes\n",
    "        model.conv = nn.Conv2d(init_features, num_classes, kernel_size=1)\n",
    "        \n",
    "        self.model = model\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "def train_multiclass_segmentation(model, train_loader, val_loader, num_epochs=50, learning_rate=1e-4):\n",
    "    # Device configuration\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model = model.to(device)\n",
    "    \n",
    "    # Loss and optimizer\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    \n",
    "    # Learning rate scheduler (optional)\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=3, factor=0.1)\n",
    "    \n",
    "    # Tensorboard for logging\n",
    "    writer = SummaryWriter('runs/multiclass_segmentation')\n",
    "    \n",
    "    # Training loop\n",
    "    best_val_loss = float('inf')\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        \n",
    "        for batch_idx, (images, masks) in enumerate(train_loader):\n",
    "            # Move to device\n",
    "            images = images.to(device)\n",
    "            masks = masks.to(device)\n",
    "            \n",
    "            # Ensure masks are long tensor for CrossEntropyLoss\n",
    "            # Assumes masks are one-hot, convert to class indices\n",
    "            masks = torch.argmax(masks, dim=1).long()\n",
    "            \n",
    "            # Zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(images)\n",
    "            \n",
    "            # Compute loss\n",
    "            loss = criterion(outputs, masks)\n",
    "            \n",
    "            # Backward pass and optimize\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss += loss.item()\n",
    "            \n",
    "            # Tensorboard logging\n",
    "            writer.add_scalar('Training Loss', loss.item(), \n",
    "                               epoch * len(train_loader) + batch_idx)\n",
    "        \n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for images, masks in val_loader:\n",
    "                images = images.to(device)\n",
    "                masks = masks.to(device)\n",
    "                \n",
    "                # Ensure masks are long tensor for CrossEntropyLoss\n",
    "                masks = torch.argmax(masks, dim=1).long()\n",
    "                \n",
    "                outputs = model(images)\n",
    "                loss = criterion(outputs, masks)\n",
    "                val_loss += loss.item()\n",
    "        \n",
    "        # Average losses\n",
    "        train_loss /= len(train_loader)\n",
    "        val_loss /= len(val_loader)\n",
    "        \n",
    "        # Learning rate scheduling\n",
    "        scheduler.step(val_loss)\n",
    "        \n",
    "        # Tensorboard logging\n",
    "        writer.add_scalar('Validation Loss', val_loss, epoch)\n",
    "        \n",
    "        # Print progress\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], '\n",
    "              f'Train Loss: {train_loss:.4f}, '\n",
    "              f'Val Loss: {val_loss:.4f}')\n",
    "        \n",
    "        # Model checkpoint\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'train_loss': train_loss,\n",
    "                'val_loss': val_loss\n",
    "            }, 'best_multiclass_unet_model.pth')\n",
    "    \n",
    "    writer.close()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import torchmetrics\n",
    "import numpy as np\n",
    "\n",
    "# Modify the model to support multiclass segmentation\n",
    "class MultiClassUNet(nn.Module):\n",
    "    def __init__(self, in_channels=3, num_classes=3, init_features=32):\n",
    "        super(MultiClassUNet, self).__init__()\n",
    "        \n",
    "        # Load the pretrained model and modify the final layer\n",
    "        model = torch.hub.load('mateuszbuda/brain-segmentation-pytorch', 'unet', \n",
    "                               in_channels=in_channels, \n",
    "                               out_channels=1, \n",
    "                               init_features=init_features, \n",
    "                               pretrained=True)\n",
    "        \n",
    "        # Replace the final convolution layer to match number of classes\n",
    "        model.conv = nn.Conv2d(init_features, num_classes, kernel_size=1)\n",
    "        \n",
    "        self.model = model\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "# Dice Loss function\n",
    "class DiceLoss(nn.Module):\n",
    "    def __init__(self, smooth=1e-6):\n",
    "        super(DiceLoss, self).__init__()\n",
    "        self.smooth = smooth\n",
    "    \n",
    "    def forward(self, outputs, targets):\n",
    "        # Apply softmax to logits to get class probabilities\n",
    "        outputs = torch.softmax(outputs, dim=1)  # (batch_size, num_classes, H, W)\n",
    "        \n",
    "        # One-hot encode the targets (if they are not one-hot already)\n",
    "        targets_onehot = torch.eye(outputs.size(1), device=outputs.device)[targets].permute(0, 3, 1, 2)\n",
    "        \n",
    "        intersection = (outputs * targets_onehot).sum(dim=(1, 2, 3))  # sum over batch, classes, height, and width\n",
    "        union = outputs.sum(dim=(1, 2, 3)) + targets_onehot.sum(dim=(1, 2, 3))  # sum over batch, classes, height, and width\n",
    "        dice_score = (2. * intersection + self.smooth) / (union + self.smooth)\n",
    "        \n",
    "        return 1 - dice_score.mean()  # Loss is 1 - Dice score\n",
    "\n",
    "\n",
    "def train_multiclass_segmentation(model, train_loader, val_loader, num_epochs=50, learning_rate=1e-4, loss_function='crossentropy'):\n",
    "    # Device configuration\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model = model.to(device)\n",
    "    \n",
    "    # Loss function choice\n",
    "    if loss_function == 'crossentropy':\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "    elif loss_function == 'dice':\n",
    "        criterion = DiceLoss()\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported loss function. Choose either 'crossentropy' or 'dice'.\")\n",
    "    \n",
    "    # Optimizer\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    \n",
    "    # Metrics\n",
    "    accuracy_metric = torchmetrics.Accuracy(task= \"multiclass\",num_classes=3).to(device)\n",
    "    iou_metric = torchmetrics.JaccardIndex(task=\"multiclass\",num_classes=3).to(device)\n",
    "    \n",
    "    # Learning rate scheduler (optional)\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=3, factor=0.1)\n",
    "    \n",
    "    # Tensorboard for logging\n",
    "    writer = SummaryWriter('runs/multiclass_segmentation')\n",
    "    \n",
    "    # Training loop\n",
    "    best_val_loss = float('inf')\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        train_loss = 0.0\n",
    "        train_accuracy = 0.0\n",
    "        train_iou = 0.0\n",
    "        \n",
    "        for batch_idx, (images, masks) in enumerate(train_loader):\n",
    "            # Move to device\n",
    "            images = images.to(device)\n",
    "            masks = masks.to(device)\n",
    "            \n",
    "            # If masks are one-hot encoded, convert them to class indices\n",
    "            if masks.ndimension() == 3:  # unbatched case\n",
    "                masks = torch.argmax(masks, dim=0).unsqueeze(0)  # Convert from (C, H, W) to (1, H, W)\n",
    "            else:\n",
    "                masks = torch.argmax(masks, dim=1)  # Convert from one-hot to class indices for batched case\n",
    "            \n",
    "            # Zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(images)\n",
    "            \n",
    "            # Compute loss\n",
    "            loss = criterion(outputs, masks)\n",
    "            \n",
    "            # Backward pass and optimize\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            # Update metrics\n",
    "            accuracy_metric.update(outputs, masks)\n",
    "            iou_metric.update(outputs, masks)\n",
    "            \n",
    "            train_loss += loss.item()\n",
    "        \n",
    "        # Compute average training loss, accuracy and IoU\n",
    "        train_loss /= len(train_loader)\n",
    "        train_accuracy = accuracy_metric.compute().item()\n",
    "        train_iou = iou_metric.compute().item()\n",
    "        \n",
    "        # Validation\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for images, masks in val_loader:\n",
    "                images = images.to(device)\n",
    "                masks = masks.to(device)\n",
    "                \n",
    "                # If masks are one-hot encoded, convert them to class indices\n",
    "                if masks.ndimension() == 3:  # unbatched case\n",
    "                    masks = torch.argmax(masks, dim=0).unsqueeze(0)  # Convert from (C, H, W) to (1, H, W)\n",
    "                else:\n",
    "                    masks = torch.argmax(masks, dim=1)  # Convert from one-hot to class indices for batched case\n",
    "                \n",
    "                outputs = model(images)\n",
    "                loss = criterion(outputs, masks)\n",
    "                val_loss += loss.item()\n",
    "        \n",
    "        # Average validation loss\n",
    "        val_loss /= len(val_loader)\n",
    "        \n",
    "        # Learning rate scheduling\n",
    "        scheduler.step(val_loss)\n",
    "        \n",
    "        # Tensorboard logging\n",
    "        writer.add_scalar('Training Loss', train_loss, epoch)\n",
    "        writer.add_scalar('Validation Loss', val_loss, epoch)\n",
    "        writer.add_scalar('Training Accuracy', train_accuracy, epoch)\n",
    "        writer.add_scalar('Training IoU', train_iou, epoch)\n",
    "        \n",
    "        # Print progress\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], '\n",
    "              f'Train Loss: {train_loss:.4f}, '\n",
    "              f'Train Accuracy: {train_accuracy:.4f}, '\n",
    "              f'Train IoU: {train_iou:.4f}, '\n",
    "              f'Val Loss: {val_loss:.4f}')\n",
    "        \n",
    "        # Model checkpoint\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'train_loss': train_loss,\n",
    "                'val_loss': val_loss\n",
    "            }, 'best_multiclass_unet_model.pth')\n",
    "    \n",
    "    writer.close()\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\zakar/.cache\\torch\\hub\\mateuszbuda_brain-segmentation-pytorch_master\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/50], Train Loss: 0.6421, Train Accuracy: 0.5154, Train IoU: 0.1784, Val Loss: 0.5813\n",
      "Epoch [2/50], Train Loss: 0.6105, Train Accuracy: 0.6677, Train IoU: 0.2295, Val Loss: 0.5688\n",
      "Epoch [3/50], Train Loss: 0.5905, Train Accuracy: 0.7318, Train IoU: 0.2511, Val Loss: 0.5515\n",
      "Epoch [4/50], Train Loss: 0.5725, Train Accuracy: 0.7695, Train IoU: 0.2645, Val Loss: 0.5382\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Example of training with CrossEntropyLoss\u001b[39;00m\n\u001b[0;32m      2\u001b[0m model \u001b[38;5;241m=\u001b[39m MultiClassUNet(in_channels\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m, num_classes\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m)\n\u001b[1;32m----> 4\u001b[0m \u001b[43mtrain_multiclass_segmentation\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_function\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdice\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[5], line 108\u001b[0m, in \u001b[0;36mtrain_multiclass_segmentation\u001b[1;34m(model, train_loader, val_loader, num_epochs, learning_rate, loss_function)\u001b[0m\n\u001b[0;32m    106\u001b[0m     \u001b[38;5;66;03m# Update metrics\u001b[39;00m\n\u001b[0;32m    107\u001b[0m     accuracy_metric\u001b[38;5;241m.\u001b[39mupdate(outputs, masks)\n\u001b[1;32m--> 108\u001b[0m     \u001b[43miou_metric\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmasks\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    110\u001b[0m     train_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n\u001b[0;32m    112\u001b[0m \u001b[38;5;66;03m# Compute average training loss, accuracy and IoU\u001b[39;00m\n",
      "File \u001b[1;32md:\\anaconda3\\envs\\iscat\\Lib\\site-packages\\torchmetrics\\metric.py:550\u001b[0m, in \u001b[0;36mMetric._wrap_update.<locals>.wrapped_func\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    548\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_enable_grad):\n\u001b[0;32m    549\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 550\u001b[0m         \u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    551\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[0;32m    552\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected all tensors to be on\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(err):\n",
      "File \u001b[1;32md:\\anaconda3\\envs\\iscat\\Lib\\site-packages\\torchmetrics\\classification\\confusion_matrix.py:285\u001b[0m, in \u001b[0;36mMulticlassConfusionMatrix.update\u001b[1;34m(self, preds, target)\u001b[0m\n\u001b[0;32m    283\u001b[0m     _multiclass_confusion_matrix_tensor_validation(preds, target, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_classes, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mignore_index)\n\u001b[0;32m    284\u001b[0m preds, target \u001b[38;5;241m=\u001b[39m _multiclass_confusion_matrix_format(preds, target, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mignore_index)\n\u001b[1;32m--> 285\u001b[0m confmat \u001b[38;5;241m=\u001b[39m \u001b[43m_multiclass_confusion_matrix_update\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpreds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_classes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    286\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfmat \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m confmat\n",
      "File \u001b[1;32md:\\anaconda3\\envs\\iscat\\Lib\\site-packages\\torchmetrics\\functional\\classification\\confusion_matrix.py:328\u001b[0m, in \u001b[0;36m_multiclass_confusion_matrix_update\u001b[1;34m(preds, target, num_classes)\u001b[0m\n\u001b[0;32m    326\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Compute the bins to update the confusion matrix with.\"\"\"\u001b[39;00m\n\u001b[0;32m    327\u001b[0m unique_mapping \u001b[38;5;241m=\u001b[39m target\u001b[38;5;241m.\u001b[39mto(torch\u001b[38;5;241m.\u001b[39mlong) \u001b[38;5;241m*\u001b[39m num_classes \u001b[38;5;241m+\u001b[39m preds\u001b[38;5;241m.\u001b[39mto(torch\u001b[38;5;241m.\u001b[39mlong)\n\u001b[1;32m--> 328\u001b[0m bins \u001b[38;5;241m=\u001b[39m \u001b[43m_bincount\u001b[49m\u001b[43m(\u001b[49m\u001b[43munique_mapping\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mminlength\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_classes\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    329\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m bins\u001b[38;5;241m.\u001b[39mreshape(num_classes, num_classes)\n",
      "File \u001b[1;32md:\\anaconda3\\envs\\iscat\\Lib\\site-packages\\torchmetrics\\utilities\\data.py:206\u001b[0m, in \u001b[0;36m_bincount\u001b[1;34m(x, minlength)\u001b[0m\n\u001b[0;32m    203\u001b[0m     mesh \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39marange(minlength, device\u001b[38;5;241m=\u001b[39mx\u001b[38;5;241m.\u001b[39mdevice)\u001b[38;5;241m.\u001b[39mrepeat(\u001b[38;5;28mlen\u001b[39m(x), \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m    204\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39meq(x\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m), mesh)\u001b[38;5;241m.\u001b[39msum(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m--> 206\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbincount\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mminlength\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mminlength\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Example of training with CrossEntropyLoss\n",
    "model = MultiClassUNet(in_channels=3, num_classes=3)\n",
    "\n",
    "train_multiclass_segmentation(model, train_loader, val_loader, num_epochs=50, loss_function='dice')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 3\n",
    "\n",
    "# Initialize the model\n",
    "model = MultiClassUNet(in_channels=3, num_classes=num_classes)\n",
    "\n",
    "# Train the model\n",
    "trained_model = train_multiclass_segmentation(\n",
    "    model, \n",
    "    train_loader, \n",
    "    val_loader, \n",
    "    num_epochs=50, \n",
    "    learning_rate=1e-4\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "iscat",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
