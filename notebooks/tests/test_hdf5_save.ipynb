{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Directory: d:\\workspace\\iscat\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.chdir(\"..\")\n",
    "print(\"Current Directory:\", os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_nd2_paths(base_path, option):\n",
    "    \"\"\"\n",
    "    Recursively collects paths to .nd2 files inside specified subfolders of Metasurface directories.\n",
    "\n",
    "    Args:\n",
    "        base_path (str): The base directory to search.\n",
    "        option (str): The folder to consider ('Brightfield' or 'Laser').\n",
    "\n",
    "    Returns:\n",
    "        list: A list of paths to .nd2 files.\n",
    "    \"\"\"\n",
    "    if option not in {'Brightfield', 'Laser'}:\n",
    "        raise ValueError(\"Option must be 'Brightfield' or 'Laser'\")\n",
    "    \n",
    "    nd2_paths = []\n",
    "    \n",
    "    for root, dirs, files in os.walk(base_path):\n",
    "        # Check if the current directory is a Metasurface directory\n",
    "        if 'Metasurface' in os.path.basename(root):\n",
    "            target_folder = os.path.join(root, option)\n",
    "            if os.path.isdir(target_folder):\n",
    "                for file in os.listdir(target_folder):\n",
    "                    if file.endswith('.nd2'):\n",
    "                        nd2_paths.append(os.path.join(target_folder, file))  \n",
    "    return nd2_paths\n",
    "a=get_nd2_paths(\"dataset\\\\2024_11_12\\Metasurface\\Chip_01\",\"Brightfield\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import h5py\n",
    "import numpy as np\n",
    "from nd2 import ND2File\n",
    "\n",
    "def nd2_to_hdf5(nd2_paths, output_hdf5_path, patch_size=(256, 256), overlap=0):\n",
    "    \"\"\"\n",
    "    Load ND2 files, extract image and mask patches, and save them into an HDF5 file with metadata.\n",
    "\n",
    "    Args:\n",
    "        nd2_paths (list of str): Paths to ND2 files.\n",
    "        output_hdf5_path (str): Path to the output HDF5 file.\n",
    "        patch_size (tuple): Size of the patches (height, width).\n",
    "        overlap (int): Overlap between patches in pixels.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    patch_height, patch_width = patch_size\n",
    "\n",
    "    # Metadata for masks\n",
    "    mask_metadata = {\n",
    "        \"Captured Cy5_mask.npy\": \"Cy5: 80nm\",\n",
    "        \"Captured FITC_mask.npy\": \"FITC: 300nm\",\n",
    "        \"Captured TRITC_mask.npy\": \"TRITC: 1300nm\",\n",
    "    }\n",
    "\n",
    "    with h5py.File(output_hdf5_path, 'w') as hdf5_file:\n",
    "        image_dataset = None  # Placeholder for image patches dataset\n",
    "        mask_dataset = None   # Placeholder for mask patches dataset\n",
    "\n",
    "        # Add general metadata to the HDF5 file\n",
    "        hdf5_file.attrs[\"description\"] = \"Image and mask patches with nanometer scale metadata.\"\n",
    "        hdf5_file.attrs[\"mask_info\"] = \", \".join([f\"class:{idx} ({value})\" for idx,(key, value) in enumerate(mask_metadata.items())])\n",
    "\n",
    "        # Process each ND2 file\n",
    "        for nd2_path in nd2_paths:\n",
    "            print(f\"Processing {nd2_path}...\")\n",
    "\n",
    "            # Load ND2 image\n",
    "            with ND2File(nd2_path) as nd2:\n",
    "                image = nd2.asarray()\n",
    "\n",
    "                # Ensure the image dimensions are correct\n",
    "                if image.ndim != 3:  # Expecting (Z, H, W)\n",
    "                    raise ValueError(f\"Expected 3D data (Z, H, W), got shape {image.shape}\")\n",
    "\n",
    "                num_slices, height, width = image.shape\n",
    "\n",
    "            # Get the directory of the current ND2 file and find corresponding masks\n",
    "            nd2_dir = os.path.dirname(nd2_path)\n",
    "            mask_paths = {name: os.path.join(nd2_dir, name) for name in mask_metadata.keys()}\n",
    "\n",
    "            # Validate that all mask files exist\n",
    "            for mask_name, mask_path in mask_paths.items():\n",
    "                if not os.path.exists(mask_path):\n",
    "                    raise FileNotFoundError(f\"Mask file {mask_path} not found.\")\n",
    "\n",
    "            # Load masks\n",
    "            masks = {mask_name: np.load(mask_path) for mask_name, mask_path in mask_paths.items()}\n",
    "\n",
    "            # Ensure masks have the same spatial dimensions as the image\n",
    "            for mask_name, mask_array in masks.items():\n",
    "                if mask_array.shape != (height, width):\n",
    "                    raise ValueError(f\"Mask {mask_name} shape {mask_array.shape} does not match image shape {(height, width)}\")\n",
    "\n",
    "            # Iterate over the image and extract patches\n",
    "            for y in range(0, height - patch_height + 1, patch_height - overlap):\n",
    "                for x in range(0, width - patch_width + 1, patch_width - overlap):\n",
    "                    # Extract image patch\n",
    "                    image_patch = image[:, y:y + patch_height, x:x + patch_width]\n",
    "\n",
    "                    # Extract mask patches\n",
    "                    mask_patches = {mask_name: mask_array[y:y + patch_height, x:x + patch_width]\n",
    "                                    for mask_name, mask_array in masks.items()}\n",
    "\n",
    "                    # Save image patches into HDF5 file\n",
    "                    if image_dataset is None:\n",
    "                        image_dataset = hdf5_file.create_dataset(\n",
    "                            \"image_patches\",\n",
    "                            shape=(0, num_slices, patch_height, patch_width),\n",
    "                            maxshape=(None, num_slices, patch_height, patch_width),\n",
    "                            chunks=(1, num_slices, patch_height, patch_width),\n",
    "                            dtype=image_patch.dtype\n",
    "                        )\n",
    "\n",
    "                    image_dataset.resize(image_dataset.shape[0] + 1, axis=0)\n",
    "                    image_dataset[-1] = image_patch\n",
    "\n",
    "                    # Save mask patches into HDF5 file\n",
    "                    if mask_dataset is None:\n",
    "                        mask_dataset = hdf5_file.create_dataset(\n",
    "                            \"mask_patches\",\n",
    "                            shape=(0, len(masks), patch_height, patch_width),\n",
    "                            maxshape=(None, len(masks), patch_height, patch_width),\n",
    "                            chunks=(1, len(masks), patch_height, patch_width),\n",
    "                            dtype=np.uint8  # Assuming masks are binary or class-labeled\n",
    "                        )\n",
    "\n",
    "                        # Add metadata for masks\n",
    "                        for idx, (mask_name, metadata) in enumerate(mask_metadata.items()):\n",
    "                            mask_dataset.attrs[f\"mask_{idx}\"] = metadata\n",
    "\n",
    "                    # Combine all masks into a single array for storage\n",
    "                    combined_mask_patch = np.stack([mask_patches[mask_name] for mask_name in masks.keys()])\n",
    "                    mask_dataset.resize(mask_dataset.shape[0] + 1, axis=0)\n",
    "                    mask_dataset[-1] = combined_mask_patch\n",
    "\n",
    "    print(f\"Image and mask patches saved to {output_hdf5_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing dataset\\2024_11_12\\Metasurface\\Chip_01\\Metasurface 01\\Brightfield\\01_01_BF.nd2...\n",
      "Image and mask patches saved to dataset\\all_patches.h5\n"
     ]
    }
   ],
   "source": [
    "import nd2\n",
    "\n",
    "nd2_files = [a[0]]  # List of ND2 file paths\n",
    "output_file = 'dataset\\\\all_patches.h5'  # Output HDF5 file\n",
    "nd2_to_hdf5_with_masks_and_metadata(nd2_files,output_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Keys: ['image_patches', 'mask_patches']\n",
      "Image patches shape: (80, 201, 256, 256)\n",
      "Mask patches shape: (80, 3, 256, 256)\n",
      "Mask metadata: ['mask_0', 'mask_1', 'mask_2']\n",
      "Description: Image and mask patches with nanometer scale metadata.\n",
      "Mask info: class:0 (Cy5: 80nm), class:1 (FITC: 300nm), class:2 (TRITC: 1300nm)\n"
     ]
    }
   ],
   "source": [
    "import h5py\n",
    "\n",
    "with h5py.File(output_file, 'r') as f:\n",
    "    print(f\"Keys: {list(f.keys())}\")\n",
    "    print(f\"Image patches shape: {f['image_patches'].shape}\")\n",
    "    print(f\"Mask patches shape: {f['mask_patches'].shape}\")\n",
    "    print(f\"Mask metadata: {list(f['mask_patches'].attrs)}\")\n",
    "    print(f\"Description: {f.attrs['description']}\")\n",
    "    print(f\"Mask info: {f.attrs['mask_info']}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "iscat",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
