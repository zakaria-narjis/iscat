{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cb878998-a2d7-46a1-8e85-754bf287bca8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Directory: /workspace/iscat\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.chdir(\"..\")\n",
    "print(\"Current Directory:\", os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9d3c1b34-f944-42df-851e-e44b0dc00e4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(41350, 16, 201)\n",
      "(array([0, 1, 2, 3]), array([32462,  8659,    60,   169]))\n"
     ]
    }
   ],
   "source": [
    "import h5py\n",
    "import numpy as np\n",
    "particle_data_path ='dataset/brightfield_particles.hdf5'\n",
    "with h5py.File(particle_data_path , 'r') as f:\n",
    "    print(f['data'].shape)\n",
    "    print(np.unique(f['labels'],return_counts=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "44bc0f64-885b-462a-9195-4cadf599f9aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import h5py\n",
    "import numpy as np\n",
    "# from torchvision.models import vit_b_16\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from tqdm import tqdm\n",
    "from torchvision.models.vision_transformer import VisionTransformer\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.metrics import confusion_matrix, f1_score, balanced_accuracy_score\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fd537f06-96fd-4319-91e9-dc4f28657147",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.transforms import v2    \n",
    "def compute_normalization_stats(h5_path, classes=None):\n",
    "    \"\"\"\n",
    "    Compute mean and standard deviation for z-score normalization.\n",
    "    \n",
    "    Args:\n",
    "        h5_path (str): Path to HDF5 file\n",
    "        classes (list, optional): List of classes to include in computation\n",
    "        \n",
    "    Returns:\n",
    "        tuple: (mean, std) computed across all data points\n",
    "    \"\"\"\n",
    "    with h5py.File(h5_path, 'r') as h5_file:\n",
    "        data = h5_file['data'][:]\n",
    "        labels = h5_file['labels'][:]\n",
    "        \n",
    "        if classes is not None:\n",
    "            # Filter data for selected classes\n",
    "            mask = np.isin(labels, classes)\n",
    "            data = data[mask]\n",
    "        \n",
    "        # Compute statistics across all dimensions\n",
    "        mean = np.mean(data)\n",
    "        std = np.std(data)\n",
    "        \n",
    "        print(f\"Computed statistics: mean = {mean:.4f}, std = {std:.4f}\")\n",
    "        \n",
    "        return mean, std\n",
    "        \n",
    "class ParticleDataset(Dataset):\n",
    "    \"\"\"Custom Dataset for particle data with flexible class selection and normalization.\"\"\"\n",
    "    def __init__(self, h5_path, classes=[0, 1], transform=None, mean=None, std=None,padding=False,indicies=list(range(0, 10000))):\n",
    "        self.h5_file = h5py.File(h5_path, 'r')\n",
    "        data = self.h5_file['data'][:]\n",
    "        labels = self.h5_file['labels'][:]\n",
    "        self.padding = padding\n",
    "        # Filter data for selected classes\n",
    "        mask = np.isin(labels, classes)\n",
    "        if indicies is None:\n",
    "            self.data = data[mask][:]\n",
    "            self.labels = labels[mask][:] \n",
    "        else:\n",
    "            self.data = data[mask][indicies]\n",
    "            self.labels = labels[mask][indicies]\n",
    "        \n",
    "        # Create class mapping to handle non-consecutive class indices\n",
    "        self.class_to_idx = {c: i for i, c in enumerate(classes)}\n",
    "        self.num_classes = len(classes)\n",
    "        \n",
    "        # Map original labels to new consecutive indices\n",
    "        self.labels = np.array([self.class_to_idx[label] for label in self.labels])\n",
    "        self.transform = transform\n",
    "        self.mean = mean\n",
    "        self.std = std\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Get particle data\n",
    "        particle = self.data[idx]  # Shape: (16, 201)\n",
    "        \n",
    "        # Apply normalization if mean and std are provided\n",
    "        if self.mean is not None and self.std is not None:\n",
    "            particle = (particle - self.mean) / self.std\n",
    "        \n",
    "        # Convert to torch tensor for better interpolation\n",
    "        particle_tensor = torch.FloatTensor(particle).unsqueeze(0)  # Add channel dim\n",
    "        \n",
    "        # Resize to (16, 16) using bicubic interpolation\n",
    "        resized = torch.nn.functional.interpolate(\n",
    "            particle_tensor.unsqueeze(0),  # Add batch dim\n",
    "            size=(224, 224),\n",
    "            mode='bicubic',\n",
    "            align_corners=True\n",
    "        ).squeeze(0).squeeze(0)  # Remove batch and channel dims\n",
    "        \n",
    "        final_tensor = resized.unsqueeze(0).repeat(3, 1, 1)  # Repeat across 3 channels\n",
    "        \n",
    "        if self.transform:\n",
    "            final_tensor = self.transform(final_tensor)\n",
    "        \n",
    "        # Create one-hot encoded label\n",
    "        label_idx = self.labels[idx]\n",
    "        label_onehot = torch.zeros(self.num_classes)\n",
    "        label_onehot[label_idx] = 1\n",
    "        \n",
    "        return final_tensor, label_onehot\n",
    "\n",
    "    def close(self):\n",
    "        self.h5_file.close()     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "eb2cddf3-50b0-451e-8d94-3c50397f6a4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "\n",
    "def distance_matrix(a, b):\n",
    "    a_expanded = a.view(-1, 1)\n",
    "    b_expanded = b.view(1, -1)\n",
    "\n",
    "    return torch.abs(a_expanded - b_expanded)\n",
    "\n",
    "def knn_divergence(points_x, points_y, k, smoothing_kernel=None):\n",
    "    xx_distances = distance_matrix(points_x, points_x)\n",
    "    xy_distances = distance_matrix(points_x, points_y) # one row for every sample in x, one col for every sample in y\n",
    "    # print(xx_distances.shape, xy_distances.shape)\n",
    "    # if the sets have different sizes\n",
    "    # e.g. y has twice as many points -> the distance to the 3rd closest point in x should be the same as the distance to the 6th point in y\n",
    "    k_multiplier = points_y.shape[0] / points_x.shape[0]\n",
    "\n",
    "    k_dist_xx = torch.sort(xx_distances, dim=1)[0][:, k]\n",
    "    k_dist_xy = torch.sort(xy_distances, dim=1)[0][:, (k * k_multiplier).to(torch.int)]\n",
    "\n",
    "    # optional: smoothen the distances \n",
    "    # (so that it matters less whether a point is the i-th or the (i+1)-th closest neighbor)\n",
    "    if smoothing_kernel != None:\n",
    "            # torch conv1d demands a channel dimension, hence the (un)squeezing\n",
    "            k_dist_xx = torch.nn.functional.conv1d(k_dist_xx.unsqueeze(1), weight=smoothing_kernel.view(1, 1, -1)).flatten(1)\n",
    "            k_dist_xy = torch.nn.functional.conv1d(k_dist_xy.unsqueeze(1), weight=smoothing_kernel.view(1, 1, -1)).flatten(1)\n",
    "\n",
    "    # return torch.mean((1 - k_dist_xx / k_dist_xy)**2)\n",
    "    return torch.mean((k_dist_xx - k_dist_xy)**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "46ced2fa-7cb7-41fb-bac6-6f267e46debb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchsummary import summary\n",
    "# resnet = torch.hub.load('pytorch/vision:v0.10.0', 'resnet18', pretrained=False)\n",
    "# # resnet .fc = nn.Linear(resnet.fc.in_features, 1)\n",
    "# resnet .fc = nn.Sequential(\n",
    "#     nn.Linear(resnet.fc.in_features, 32),\n",
    "#     nn.ReLU(),\n",
    "#     nn.Linear(32, 1))\n",
    "# summary(resnet, input_size=(3, 16, 201),device='cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "93d414e4-aee4-42ef-a922-7bbda00008f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda:11\n",
      "Computed statistics: mean = 7407.4357, std = 1323.8027\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "DEVICE = \"cuda:11\"\n",
    "# Device configuration\n",
    "device = torch.device(DEVICE if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "# classes = [0]\n",
    "transform = v2.Compose([\n",
    "    v2.RandomVerticalFlip(p=0.5),\n",
    "    v2.RandomHorizontalFlip(p=0.5),\n",
    "])\n",
    "mean, std = compute_normalization_stats('dataset/brightfield_particles.hdf5', classes=[0])\n",
    "dataset_80 = ParticleDataset(h5_path='dataset/brightfield_particles.hdf5',\n",
    "                          classes=[0],\n",
    "                          mean=mean,\n",
    "                          std=std,\n",
    "                          padding=True,\n",
    "                             transform = transform,\n",
    "                             indicies=list(range(0, 20000)),\n",
    "                         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "53312d91-ce1f-48ed-b639-22384bdd3d87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def compute_contrast(img):\n",
    "#     return (img.max()-img.min())/img.mean()\n",
    "# img = dataset_80[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f944d584-0830-44af-a01b-a390e3f4a120",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "def generate_label_distribution(num_points=10000, mean=76, std=22.5, min_value=10, max_value=None):\n",
    "    \"\"\"\n",
    "    Generate a tensor of points sampled from a normal distribution with specified mean and standard deviation\n",
    "    while rejecting points outside the optional min and max value constraints.\n",
    "    \n",
    "    Args:\n",
    "        num_points (int): Number of points to generate\n",
    "        mean (float): Mean of the distribution\n",
    "        std (float): Standard deviation of the distribution\n",
    "        min_value (float, optional): Minimum value of the distribution (inclusive)\n",
    "        max_value (float, optional): Maximum value of the distribution (inclusive)\n",
    "    \n",
    "    Returns:\n",
    "        torch.Tensor: Tensor of generated points within the specified range\n",
    "    \"\"\"\n",
    "    points = torch.empty(0)  # Initialize an empty tensor to store valid points\n",
    "\n",
    "    while points.numel() < num_points:\n",
    "        # Generate points from normal distribution\n",
    "        generated_points = torch.normal(mean=mean, std=std, size=(num_points,))\n",
    "        \n",
    "        # Filter points based on the min and max values\n",
    "        if min_value is not None:\n",
    "            generated_points = generated_points[generated_points >= min_value]\n",
    "        if max_value is not None:\n",
    "            generated_points = generated_points[generated_points <= max_value]\n",
    "        \n",
    "        # Add the valid points to the tensor\n",
    "        points = torch.cat((points, generated_points))\n",
    "    # Return only the first `num_points` points\n",
    "    return points[:num_points]\n",
    "    \n",
    "batch_size = 10000\n",
    "num_points = 10000\n",
    "dataloader_80 = DataLoader(dataset_80, batch_size=batch_size, shuffle=True)\n",
    "label_points_80 = generate_label_distribution(num_points, mean=76, std=22.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "91e9fc85-2b2f-4665-a1aa-b0d9c9a19d38",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_resnet(model, dataloaders, label_points, device, num_epochs=10, learning_rate=3e-2):\n",
    "    \"\"\"\n",
    "    Train ResNet model using KNN divergence loss with early stopping and learning rate scheduling.\n",
    "    \n",
    "    Args:\n",
    "        model (torch.nn.Module): ResNet model\n",
    "        dataloader (torch.utils.data.DataLoader): Training dataloader\n",
    "        label_points (torch.Tensor): Pre-generated label points\n",
    "        device (torch.device): Device to train on\n",
    "        num_epochs (int): Number of training epochs\n",
    "        learning_rate (float): Learning rate for optimizer\n",
    "    \n",
    "    Returns:\n",
    "        model: Trained model\n",
    "        best_loss: Best training loss achieved\n",
    "    \"\"\"\n",
    "    # Move label points to the specified device\n",
    "    labels = [label.to(device, non_blocking=True) for label in label_points] \n",
    "    # labels_80, labels_300 = labels[0],labels[1] \n",
    "    # Prepare k values for KNN divergence\n",
    "    ks = [torch.arange(2, label_point.shape[0]//10, dtype=torch.int) for label_point in label_points]    \n",
    "    # Setup optimizer\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    \n",
    "    # Setup learning rate scheduler with patience of 8\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        optimizer, mode='min', patience=8, factor=0.5\n",
    "    )\n",
    "    \n",
    "    model.to(device)\n",
    "   \n",
    "    # Early stopping parameters\n",
    "    best_loss = float('inf')\n",
    "    patience = 20\n",
    "    patience_counter = 0\n",
    "    best_model_state = None\n",
    "    \n",
    "    # Training loop\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        batch_count = 0\n",
    "        \n",
    "        for idx, (label, dataloader) in enumerate(zip(labels, dataloaders)):\n",
    "                for batch_images, _ in dataloader:\n",
    "                    batch_count += 1\n",
    "                    gt = torch.clone(label)\n",
    "                    batch_images = batch_images.to(device)\n",
    "                    \n",
    "                    # Zero gradients\n",
    "                    optimizer.zero_grad()\n",
    "                    \n",
    "                    # Forward pass: generate predictions\n",
    "                    batch_predictions = model(batch_images)\n",
    "                    \n",
    "                    # Compute KNN divergence loss\n",
    "                    loss = knn_divergence(batch_predictions, gt, ks[idx])\n",
    "                    \n",
    "                    # Backward pass and optimize\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "                    \n",
    "                    total_loss += loss.item()\n",
    "        \n",
    "        # Calculate average loss for the epoch\n",
    "        avg_loss = total_loss / batch_count\n",
    "        current_lr = optimizer.param_groups[0]['lr']\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {avg_loss:.4f}, LR: {current_lr:.2e}')\n",
    "        \n",
    "        # Learning rate scheduling\n",
    "        scheduler.step(avg_loss)\n",
    "        \n",
    "        # Early stopping check\n",
    "        if avg_loss < best_loss:\n",
    "            best_loss = avg_loss\n",
    "            patience_counter = 0\n",
    "            best_model_state = model.state_dict().copy()\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= patience:\n",
    "                print(f'Early stopping triggered after {epoch + 1} epochs')\n",
    "                # Restore best model\n",
    "                model.load_state_dict(best_model_state)\n",
    "                break\n",
    "    \n",
    "    return model, best_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9b31a67-7dae-46ea-8df3-9e59d59a8d7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /root/.cache/torch/hub/pytorch_vision_v0.10.0\n"
     ]
    }
   ],
   "source": [
    "resnet = torch.hub.load('pytorch/vision:v0.10.0', 'resnet18', weights=None)\n",
    "dataloaders = (dataloader_80,)\n",
    "label_points = (label_points_80,)\n",
    "# resnet .fc = nn.Linear(resnet.fc.in_features, 1)\n",
    "resnet .fc = nn.Sequential(\n",
    "    nn.Linear(resnet.fc.in_features, 128),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(128, 64),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(64, 1)\n",
    ")\n",
    "resnet,best_loss = train_resnet(resnet, dataloaders, label_points, device, num_epochs=300)\n",
    "resnet.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb50403a-d1fb-48db-8f94-bd4ecdc2cb39",
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet.eval()\n",
    "resnet(dataset_80[0][0].unsqueeze(0).to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95f6ed8f-4812-4f11-bf7f-dfacc6aa106b",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    a= next(iter(dataloader_80))[0]\n",
    "    out  = resnet(a.to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feee442d-c61a-45b2-babd-1d9639491032",
   "metadata": {},
   "outputs": [],
   "source": [
    "out.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb0433e3-4879-4299-803b-9b70aa230b7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot histograms\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.hist(label_points_80 , bins=50, alpha=0.6, label='Ground Truth', color='blue', density=True)\n",
    "plt.hist(out.cpu().detach().numpy(), bins=50, alpha=0.6, label='Prediction', color='red', density=True)\n",
    "\n",
    "# Labels and legend\n",
    "plt.xlabel('Diamerter[nm]')\n",
    "plt.ylabel('Density[norm.]')\n",
    "plt.title('Ground Truth vs Prediction Distribution_80nm')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "# Show plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f0ec387-8e2c-49ab-af56-44bd3b8fe702",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_dataset_300 = ParticleDataset(h5_path='dataset/brightfield_particles.hdf5',\n",
    "                          classes=[1],\n",
    "                          mean=mean,\n",
    "                          std=std,\n",
    "                          padding=True,\n",
    "                        transform = None,\n",
    "                        indicies = None\n",
    "                         )\n",
    "plot_dataloader_300 = DataLoader(plot_dataset_300 , batch_size=len(plot_dataset_300))\n",
    "with torch.no_grad():\n",
    "    out_2 = next(iter(plot_dataloader_300))[0]\n",
    "    out_2  = resnet(out_2.to(device)).cpu().detach().numpy()\n",
    "label_points_300 = generate_label_distribution(len(plot_dataset_300), mean=302, std=25)\n",
    "print(out_2.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c532407c-9edd-45b3-bdc7-6ce915615c45",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_contrast(imgs:torch.Tensor,dim=(1, 2, 3)):\n",
    "    \"\"\"\n",
    "        args:\n",
    "            imgs:batch_images\n",
    "            dim: dim\n",
    "        return\n",
    "            output: contrast value of each image in the batch\n",
    "    \"\"\"\n",
    "    return (imgs.amax(dim=dim)-img.amax(dim=dim))/img.mean(dim=dim)\n",
    "out\n",
    "with torch.no_grad():\n",
    "    a = next(iter(dataloader_80))[0]\n",
    "    c = compute_contrast(a)\n",
    "    sorted_indices = torch.argsort(c) \n",
    "    a=a[sorted_indices]\n",
    "    out = resnet(a.to(device))\n",
    "plt.scatter(list(range(0,len(out[0:1000]))),out[0:1000].cpu())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a883cd88-2497-40e1-9b8d-910f5ab1d665",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "# Example list of values\n",
    "# values =out_2+(302-out_2.mean())\n",
    "values =out_2\n",
    "# values = o\n",
    "# Set up the plot\n",
    "plt.figure(figsize=(8, 6))\n",
    "\n",
    "# Plot the histogram\n",
    "plt.hist(values, bins=50, color='red',label='Prediction',alpha=0.7, density=True)\n",
    "plt.hist(label_points_300  , bins=50, alpha=0.6, label='Ground Truth', color='blue', density=True)\n",
    "# Add labels\n",
    "# Labels and legend\n",
    "plt.xlabel('Diamerter[nm]')\n",
    "plt.ylabel('Density[norm.]')\n",
    "plt.title('Ground Truth vs Prediction Distribution_300nm')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "# Show plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "721d3d88-0e70-4e1e-aad6-bf5685e7333c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_dataset_1300 = ParticleDataset(h5_path='dataset/brightfield_particles.hdf5',\n",
    "                          classes=[2],\n",
    "                          mean=mean,\n",
    "                          std=std,\n",
    "                          padding=True,\n",
    "                        transform = None,\n",
    "                        indicies=None\n",
    "                         )\n",
    "plot_dataloader_1300 = DataLoader(plot_dataset_1300, batch_size=len(plot_dataset_1300))\n",
    "with torch.no_grad():\n",
    "    out_3 = next(iter(plot_dataloader_1300))[0]\n",
    "    out_3  = resnet(out_3.to(device)).cpu().detach().numpy()\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Example list of values\n",
    "values = out_3\n",
    "# values = o\n",
    "# Set up the plot\n",
    "plt.figure(figsize=(8, 6))\n",
    "\n",
    "# Plot the histogram\n",
    "plt.hist(values, bins=100, density=True, color='blue', alpha=0.7)\n",
    "\n",
    "# Add labels\n",
    "plt.title('Distribution of Values_1300nm')\n",
    "plt.xlabel('Value')\n",
    "plt.ylabel('Density')\n",
    "\n",
    "# Show plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97fb1c84-8237-4268-8cd7-be87607073d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_dataset_80 = ParticleDataset(h5_path='dataset/brightfield_particles.hdf5',\n",
    "                          classes=[0],\n",
    "                          mean=mean,\n",
    "                          std=std,\n",
    "                          padding=True,\n",
    "                            transform = None,indicies=list(range(0, 20001))\n",
    "                         )\n",
    "plot_dataloader_80 = DataLoader(plot_dataset_80, batch_size=20000)\n",
    "with torch.no_grad():\n",
    "    out_4 = next(iter(plot_dataloader_80))[0]\n",
    "    out_4  = resnet(out_4.to(device)).cpu().detach().numpy()\n",
    "import matplotlib.pyplot as plt\n",
    "# Example list of values\n",
    "values = out_4\n",
    "# values = o\n",
    "# Set up the plot\n",
    "plt.figure(figsize=(8, 6))\n",
    "\n",
    "# Plot the histogram\n",
    "plt.hist(values, bins=100, density=True, color='blue', alpha=0.7)\n",
    "\n",
    "# Add labels\n",
    "plt.title('Distribution of Values_80nm')\n",
    "plt.xlabel('Value')\n",
    "plt.ylabel('Density')\n",
    "\n",
    "# Show plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "230d8064-0bdd-4a18-9fec-61148331e950",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    output_map = resnet.conv1(dataset_80[0][0].unsqueeze(0).to(device)).squeeze(0).cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97cc3170-d622-4221-8449-cf2f6a915198",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(18, 10))\n",
    "f=torch.clone(output_map[3])\n",
    "# f[f<(f.mean())]=0\n",
    "ax.imshow(f,cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aa5d029-fd07-40ff-9bd5-66a626d800d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    imgs = next(iter(dataloader_80))[0]  # (10000,3,16,201)\n",
    "    img = imgs[2000].unsqueeze(0)  # (1,3,16,201)\n",
    "    \n",
    "    # Get prediction for original image\n",
    "    size_1 = resnet(img.to(device)).cpu()\n",
    "    size_1 = size_1.squeeze(0)\n",
    "    \n",
    "    # Flip the image horizontally (along the last dimension)\n",
    "    img_flipped = torch.flip(img, dims=[-1])\n",
    "    \n",
    "    # Get prediction for flipped image\n",
    "    size_2 = resnet(img_flipped.to(device)).cpu()\n",
    "    size_2 = size_2.squeeze(0)\n",
    "    \n",
    "    # Print both predictions\n",
    "    print(f\"Original image size prediction: {size_1.item():.3f}\")\n",
    "    print(f\"Flipped image size prediction: {size_2.item():.3f}\")\n",
    "    print(f\"Absolute difference: {abs(size_1.item() - size_2.item()):.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f220a167-dfa9-4632-8562-3b899edca031",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plot_dataset_80 = ParticleDataset(h5_path='dataset/brightfield_particles.hdf5',\n",
    "                          classes=[0],\n",
    "                          mean=mean,\n",
    "                          std=std,\n",
    "                          padding=True,\n",
    "                            transform = None\n",
    "                         )\n",
    "plot_dataloader = DataLoader(plot_dataset_80, batch_size=batch_size)\n",
    "with torch.no_grad():\n",
    "    imgs = next(iter(plot_dataloader))[0]  # (10000,3,16,201)\n",
    "    sizes = resnet(imgs.to(device)).cpu() \n",
    "    \n",
    "max_size, max_idx = sizes.max(dim=0)\n",
    "min_size, min_idx = sizes.min(dim=0)\n",
    "\n",
    "# Compute middle size (median)\n",
    "mid_size = sizes.median()\n",
    "mid_idx = (sizes - mid_size).abs().argmin()\n",
    "mid_idx = torch.tensor([mid_idx], dtype=torch.int64)\n",
    "\n",
    "# Create 9 intermediate values between min, mid, and max\n",
    "intermediate_sizes, intermediate_indices = [], []\n",
    "for fraction in torch.linspace(0, 1, steps=9):\n",
    "    interp_size = min_size + fraction * (max_size - min_size)\n",
    "    closest_idx = (sizes - interp_size).abs().argmin()\n",
    "    intermediate_sizes.append(sizes[closest_idx])\n",
    "    intermediate_indices.append(closest_idx)\n",
    "\n",
    "# Convert indices to tensor\n",
    "intermediate_indices = torch.tensor(intermediate_indices, dtype=torch.int64)\n",
    "\n",
    "# Get images and resize them\n",
    "resized_images = [\n",
    "    torch.nn.functional.interpolate(\n",
    "        imgs[idx][0:1].unsqueeze(0), size=(16, 32), mode=\"bilinear\", align_corners=False\n",
    "    ).squeeze(0)[0]\n",
    "    for idx in intermediate_indices\n",
    "]\n",
    "\n",
    "# Create 3x3 subplot\n",
    "fig, axes = plt.subplots(3, 3, figsize=(12, 9))\n",
    "\n",
    "# Plot images\n",
    "for ax, img, size in zip(axes.flat, resized_images, intermediate_sizes):\n",
    "    ax.imshow(img, cmap=\"gray\")\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "    ax.title.set_text(f\"size: {size.item()}\")\n",
    "\n",
    "# Adjust layout and show\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "iscat",
   "language": "python",
   "name": "iscat"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
