{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Directory: /workspace/iscat\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.chdir(\"..\")\n",
    "print(\"Current Directory:\", os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_fluo_paths(root_path:str, mode:str=\"Brightfield\"):\n",
    "    \"\"\"\n",
    "    Extract paths to .nd2 files and corresponding TIFF files from the specified mode folder.\n",
    "\n",
    "    Args:\n",
    "        root_path (str): The root directory to search.\n",
    "        mode (str): The folder name to focus on (default is 'Brightfield').\n",
    "\n",
    "    Returns:\n",
    "        tuple: Two lists - list of .nd2 file paths and list of tuples with corresponding TIFF file paths.\n",
    "    \"\"\"\n",
    "\n",
    "    target_files = []\n",
    "\n",
    "    for dirpath, dirnames, filenames in os.walk(root_path):\n",
    "        if os.path.basename(dirpath) == mode:\n",
    "\n",
    "            # Generate TIFF file paths dynamically based on the prefix\n",
    "            cy5_path = os.path.join(dirpath, f'Captured Cy5.tif')\n",
    "            fitc_path = os.path.join(dirpath, f'Captured FITC.tif')\n",
    "            tritc_path = os.path.join(dirpath, f'Captured TRITC.tif')\n",
    "            target_files.append((cy5_path, fitc_path, tritc_path))\n",
    "            \n",
    "            # Ensure all three TIFF files exist\n",
    "            assert all(os.path.exists(path) for path in [cy5_path, fitc_path, tritc_path])\n",
    "        \n",
    "            \n",
    "    return target_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = os.path.join(\"data\", \"2024_11_29\",\"Metasurface\",\"Chip_02\")\n",
    "target_files= get_fluo_paths(base_path, 'Brightfield')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('data/2024_11_29/Metasurface/Chip_02/Metasurface 01/Brightfield/Captured Cy5.tif',\n",
       "  'data/2024_11_29/Metasurface/Chip_02/Metasurface 01/Brightfield/Captured FITC.tif',\n",
       "  'data/2024_11_29/Metasurface/Chip_02/Metasurface 01/Brightfield/Captured TRITC.tif'),\n",
       " ('data/2024_11_29/Metasurface/Chip_02/Metasurface 09/Brightfield/Captured Cy5.tif',\n",
       "  'data/2024_11_29/Metasurface/Chip_02/Metasurface 09/Brightfield/Captured FITC.tif',\n",
       "  'data/2024_11_29/Metasurface/Chip_02/Metasurface 09/Brightfield/Captured TRITC.tif'),\n",
       " ('data/2024_11_29/Metasurface/Chip_02/Metasurface 05/Brightfield/Captured Cy5.tif',\n",
       "  'data/2024_11_29/Metasurface/Chip_02/Metasurface 05/Brightfield/Captured FITC.tif',\n",
       "  'data/2024_11_29/Metasurface/Chip_02/Metasurface 05/Brightfield/Captured TRITC.tif'),\n",
       " ('data/2024_11_29/Metasurface/Chip_02/Metasurface 07/Brightfield/Captured Cy5.tif',\n",
       "  'data/2024_11_29/Metasurface/Chip_02/Metasurface 07/Brightfield/Captured FITC.tif',\n",
       "  'data/2024_11_29/Metasurface/Chip_02/Metasurface 07/Brightfield/Captured TRITC.tif'),\n",
       " ('data/2024_11_29/Metasurface/Chip_02/Metasurface 02/Brightfield/Captured Cy5.tif',\n",
       "  'data/2024_11_29/Metasurface/Chip_02/Metasurface 02/Brightfield/Captured FITC.tif',\n",
       "  'data/2024_11_29/Metasurface/Chip_02/Metasurface 02/Brightfield/Captured TRITC.tif'),\n",
       " ('data/2024_11_29/Metasurface/Chip_02/Metasurface 08/Brightfield/Captured Cy5.tif',\n",
       "  'data/2024_11_29/Metasurface/Chip_02/Metasurface 08/Brightfield/Captured FITC.tif',\n",
       "  'data/2024_11_29/Metasurface/Chip_02/Metasurface 08/Brightfield/Captured TRITC.tif'),\n",
       " ('data/2024_11_29/Metasurface/Chip_02/Metasurface 06/Brightfield/Captured Cy5.tif',\n",
       "  'data/2024_11_29/Metasurface/Chip_02/Metasurface 06/Brightfield/Captured FITC.tif',\n",
       "  'data/2024_11_29/Metasurface/Chip_02/Metasurface 06/Brightfield/Captured TRITC.tif'),\n",
       " ('data/2024_11_29/Metasurface/Chip_02/Metasurface 04/Brightfield/Captured Cy5.tif',\n",
       "  'data/2024_11_29/Metasurface/Chip_02/Metasurface 04/Brightfield/Captured FITC.tif',\n",
       "  'data/2024_11_29/Metasurface/Chip_02/Metasurface 04/Brightfield/Captured TRITC.tif'),\n",
       " ('data/2024_11_29/Metasurface/Chip_02/Metasurface 03/Brightfield/Captured Cy5.tif',\n",
       "  'data/2024_11_29/Metasurface/Chip_02/Metasurface 03/Brightfield/Captured FITC.tif',\n",
       "  'data/2024_11_29/Metasurface/Chip_02/Metasurface 03/Brightfield/Captured TRITC.tif')]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating Masks with ComDet: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████| 9/9 [06:42<00:00, 44.76s/it]\n"
     ]
    }
   ],
   "source": [
    "from src.data_processing.utils import Utils\n",
    "Utils.generate_np_masks(target_files,seg_args=None,seg_method=\"comdet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_items([('a', 1), ('c', 3), ('b', 2)])\n"
     ]
    }
   ],
   "source": [
    "a={\n",
    "    \"a\":1,\n",
    "    \"c\":3,\n",
    "    \"b\":2,\n",
    "    \n",
    "\n",
    "}\n",
    "print(a.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import numpy as np\n",
    "import torch\n",
    "from typing import Tuple, Dict, List\n",
    "\n",
    "def convert_instance_masks_to_maskrcnn_format(\n",
    "    instance_masks: np.ndarray\n",
    ") -> Tuple[List[torch.Tensor], List[torch.Tensor], List[torch.Tensor]]:\n",
    "    \"\"\"\n",
    "    Convert instance segmentation masks to MaskRCNN format.\n",
    "    \n",
    "    Args:\n",
    "        instance_masks: Array of shape (C, H, W) where C is number of classes,\n",
    "                       each channel contains instance masks with unique IDs\n",
    "    \n",
    "    Returns:\n",
    "        Tuple containing:\n",
    "        - masks: List of binary masks for each instance\n",
    "        - boxes: List of bounding boxes in [x1, y1, x2, y2] format\n",
    "        - labels: List of class labels for each instance\n",
    "    \"\"\"\n",
    "    height, width = instance_masks.shape[1:]\n",
    "    masks = []\n",
    "    boxes = []\n",
    "    labels = []\n",
    "    \n",
    "    # Process each class\n",
    "    for class_idx in range(instance_masks.shape[0]):\n",
    "        class_mask = instance_masks[class_idx]\n",
    "        unique_instances = np.unique(class_mask)\n",
    "        \n",
    "        # Skip background (0)\n",
    "        for instance_id in unique_instances[unique_instances > 0]:\n",
    "            # Create binary mask for this instance\n",
    "            binary_mask = (class_mask == instance_id)\n",
    "            masks.append(torch.from_numpy(binary_mask))\n",
    "            \n",
    "            # Get bounding box\n",
    "            y_indices, x_indices = np.where(binary_mask)\n",
    "            if len(y_indices) > 0 and len(x_indices) > 0:\n",
    "                x1, x2 = x_indices.min(), x_indices.max()\n",
    "                y1, y2 = y_indices.min(), y_indices.max()\n",
    "                boxes.append(torch.tensor([x1, y1, x2, y2], dtype=torch.float32))\n",
    "                \n",
    "                # Add class label (add 1 because MaskRCNN uses 0 as background)\n",
    "                labels.append(torch.tensor(class_idx + 1))\n",
    "    \n",
    "    # Handle empty cases\n",
    "    if not masks:\n",
    "        return [], torch.zeros((0, 4)), torch.zeros(0)\n",
    "    \n",
    "    return masks, torch.stack(boxes), torch.stack(labels)\n",
    "\n",
    "def process_image_for_maskrcnn(\n",
    "    image: np.ndarray,\n",
    "    normalize: bool = True\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Process the image for MaskRCNN input.\n",
    "    \n",
    "    Args:\n",
    "        image: numpy array of shape (Z, H, W)\n",
    "        normalize: whether to normalize the image\n",
    "    \n",
    "    Returns:\n",
    "        torch.Tensor: Processed image tensor\n",
    "    \"\"\"\n",
    "    # Convert to torch tensor\n",
    "    image = torch.from_numpy(image).float()\n",
    "    \n",
    "    # Handle Z-stack: either take max projection or specific slice\n",
    "    if image.ndim == 3:\n",
    "        image = image.max(dim=0)[0]  # max projection\n",
    "        # Alternatively: image = image[middle_slice]\n",
    "    \n",
    "    # Add channel dimension\n",
    "    image = image.unsqueeze(0)\n",
    "    \n",
    "    # Normalize if requested\n",
    "    if normalize:\n",
    "        image = (image - image.mean()) / (image.std() + 1e-7)\n",
    "    \n",
    "    return image\n",
    "\n",
    "def load_sample_from_hdf5(\n",
    "    hdf5_path: str,\n",
    "    idx: int,\n",
    "    normalize_image: bool = True\n",
    ") -> Tuple[torch.Tensor, Dict]:\n",
    "    \"\"\"\n",
    "    Load a single sample from HDF5 file in MaskRCNN format.\n",
    "    \n",
    "    Args:\n",
    "        hdf5_path: Path to HDF5 file\n",
    "        idx: Index of the sample to load\n",
    "        normalize_image: Whether to normalize the image\n",
    "    \n",
    "    Returns:\n",
    "        Tuple containing:\n",
    "        - image: Tensor of shape (C, H, W)\n",
    "        - target: Dictionary containing:\n",
    "            - boxes: Tensor of shape (N, 4) - coordinates of N bounding boxes\n",
    "            - labels: Tensor of shape (N,) - class labels for each box\n",
    "            - masks: List of N binary masks\n",
    "            - image_id: Tensor containing image index\n",
    "            - area: Tensor of shape (N,) - area of each mask\n",
    "            - iscrowd: Tensor of shape (N,) - set to 0 for all instances\n",
    "    \"\"\"\n",
    "    with h5py.File(hdf5_path, 'r') as f:\n",
    "        # Load image and instance masks\n",
    "        image = f['image_patches'][idx]\n",
    "        instance_masks = f['instance_masks'][idx]\n",
    "    \n",
    "    # Process image\n",
    "    processed_image = process_image_for_maskrcnn(image, normalize_image)\n",
    "    \n",
    "    # Convert instance masks to MaskRCNN format\n",
    "    masks, boxes, labels = convert_instance_masks_to_maskrcnn_format(instance_masks)\n",
    "    \n",
    "    # Calculate areas for each instance\n",
    "    if masks:\n",
    "        areas = torch.tensor([mask.sum().float() for mask in masks])\n",
    "        iscrowd = torch.zeros((len(masks),), dtype=torch.int64)\n",
    "    else:\n",
    "        areas = torch.zeros((0,), dtype=torch.float32)\n",
    "        iscrowd = torch.zeros((0,), dtype=torch.int64)\n",
    "    \n",
    "    # Create target dictionary\n",
    "    target = {\n",
    "        'boxes': boxes,\n",
    "        'labels': labels,\n",
    "        'masks': masks,\n",
    "        'image_id': torch.tensor([idx]),\n",
    "        'area': areas,\n",
    "        'iscrowd': iscrowd\n",
    "    }\n",
    "    \n",
    "    return processed_image, target\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_image, target= load_sample_from_hdf5('dataset/brightfield_instance.hdf5',0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "with h5py.File(path, 'r') as f:\n",
    "    # Load image and instance masks\n",
    "    image = f['image_patches'][0]\n",
    "    instance_masks = f['instance_masks'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0]],\n",
       "\n",
       "       [[0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0]],\n",
       "\n",
       "       [[0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0]]], dtype=uint8)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "instance_masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'dataset/brightfield_instance.hdf5'\n",
    "def load_sample_from_hdf5(file_path, index):\n",
    "    with h5py.File(file_path, 'r') as f:\n",
    "        image = f['images'][index]  # Assuming 'images' is the dataset key\n",
    "        boxes = f['boxes'][index]   # (N, 4) format: [x_min, y_min, x_max, y_max]\n",
    "        labels = f['labels'][index] # Class labels for each instance\n",
    "        masks = f['masks'][index]   # Binary masks (N, H, W)\n",
    "        \n",
    "    return image, {'boxes': boxes, 'labels': labels, 'masks': masks}\n",
    "def plot_instance_segmentation(image, target):\n",
    "    # Normalize the 16-bit image for visualization\n",
    "    image = image.astype(np.float32) / (2**16)\n",
    "    \n",
    "    # Convert grayscale to RGB for visualization\n",
    "    image_rgb = cv2.cvtColor((image * 255).astype(np.uint8), cv2.COLOR_GRAY2RGB)\n",
    "    \n",
    "    # Get segmentation data\n",
    "    boxes = target['boxes']\n",
    "    labels = target['labels']\n",
    "    masks = target['masks']\n",
    "    \n",
    "    plt.figure(figsize=(8, 8))\n",
    "    plt.imshow(image_rgb)\n",
    "    \n",
    "    # Random color generator\n",
    "    def random_color():\n",
    "        return tuple(random.choices(range(256), k=3))\n",
    "    \n",
    "    for i in range(len(boxes)):\n",
    "        x_min, y_min, x_max, y_max = boxes[i]\n",
    "        color = random_color()\n",
    "        \n",
    "        # Draw bounding box\n",
    "        cv2.rectangle(image_rgb, (int(x_min), int(y_min)), (int(x_max), int(y_max)), color, 2)\n",
    "        plt.text(x_min, y_min - 5, f'Class {labels[i]}', color='yellow', fontsize=8, bbox=dict(facecolor='black', alpha=0.5))\n",
    "        \n",
    "        # Overlay the segmentation mask with transparency\n",
    "        mask = masks[i].astype(np.uint8)\n",
    "        colored_mask = np.zeros_like(image_rgb)\n",
    "        colored_mask[:, :, 0] = mask * color[0]  # Red channel\n",
    "        colored_mask[:, :, 1] = mask * color[1]  # Green channel\n",
    "        colored_mask[:, :, 2] = mask * color[2]  # Blue channel\n",
    "        \n",
    "        image_rgb = cv2.addWeighted(image_rgb, 1, colored_mask, 0.5, 0)\n",
    "    \n",
    "    plt.imshow(image_rgb)\n",
    "    plt.axis(\"off\")\n",
    "    plt.title(\"Instance Segmentation Visualization\")\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"Unable to synchronously open object (object 'images' doesn't exist)\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[26], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m processed_image, target \u001b[38;5;241m=\u001b[39m load_sample_from_hdf5(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdataset/brightfield_instance.hdf5\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m      2\u001b[0m plot_instance_segmentation(processed_image, target)\n",
      "Cell \u001b[0;32mIn[25], line 4\u001b[0m, in \u001b[0;36mload_sample_from_hdf5\u001b[0;34m(file_path, index)\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_sample_from_hdf5\u001b[39m(file_path, index):\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m h5py\u001b[38;5;241m.\u001b[39mFile(file_path, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m----> 4\u001b[0m         image \u001b[38;5;241m=\u001b[39m f[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimages\u001b[39m\u001b[38;5;124m'\u001b[39m][index]  \u001b[38;5;66;03m# Assuming 'images' is the dataset key\u001b[39;00m\n\u001b[1;32m      5\u001b[0m         boxes \u001b[38;5;241m=\u001b[39m f[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mboxes\u001b[39m\u001b[38;5;124m'\u001b[39m][index]   \u001b[38;5;66;03m# (N, 4) format: [x_min, y_min, x_max, y_max]\u001b[39;00m\n\u001b[1;32m      6\u001b[0m         labels \u001b[38;5;241m=\u001b[39m f[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m'\u001b[39m][index] \u001b[38;5;66;03m# Class labels for each instance\u001b[39;00m\n",
      "File \u001b[0;32mh5py/_objects.pyx:54\u001b[0m, in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mh5py/_objects.pyx:55\u001b[0m, in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/miniconda3/envs/iscat/lib/python3.11/site-packages/h5py/_hl/group.py:357\u001b[0m, in \u001b[0;36mGroup.__getitem__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    355\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid HDF5 object reference\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    356\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(name, (\u001b[38;5;28mbytes\u001b[39m, \u001b[38;5;28mstr\u001b[39m)):\n\u001b[0;32m--> 357\u001b[0m     oid \u001b[38;5;241m=\u001b[39m h5o\u001b[38;5;241m.\u001b[39mopen(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mid, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_e(name), lapl\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lapl)\n\u001b[1;32m    358\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    359\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAccessing a group is done with bytes or str, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    360\u001b[0m                     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnot \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\u001b[38;5;28mtype\u001b[39m(name)))\n",
      "File \u001b[0;32mh5py/_objects.pyx:54\u001b[0m, in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mh5py/_objects.pyx:55\u001b[0m, in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mh5py/h5o.pyx:257\u001b[0m, in \u001b[0;36mh5py.h5o.open\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: \"Unable to synchronously open object (object 'images' doesn't exist)\""
     ]
    }
   ],
   "source": [
    "processed_image, target = load_sample_from_hdf5('dataset/brightfield_instance.hdf5', 0)\n",
    "plot_instance_segmentation(processed_image, target)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "iscat",
   "language": "python",
   "name": "iscat"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
