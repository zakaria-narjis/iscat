{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cb878998-a2d7-46a1-8e85-754bf287bca8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Directory: /workspace/iscat\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.chdir(\"..\")\n",
    "print(\"Current Directory:\", os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9d3c1b34-f944-42df-851e-e44b0dc00e4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(41350, 16, 201)\n",
      "(array([0, 1, 2, 3]), array([32462,  8659,    60,   169]))\n"
     ]
    }
   ],
   "source": [
    "import h5py\n",
    "import numpy as np\n",
    "particle_data_path ='dataset/brightfield_particles.hdf5'\n",
    "with h5py.File(particle_data_path , 'r') as f:\n",
    "    print(f['data'].shape)\n",
    "    print(np.unique(f['labels'],return_counts=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "44bc0f64-885b-462a-9195-4cadf599f9aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import h5py\n",
    "import numpy as np\n",
    "# from torchvision.models import vit_b_16\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from tqdm import tqdm\n",
    "from torchvision.models.vision_transformer import VisionTransformer\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.metrics import confusion_matrix, f1_score, balanced_accuracy_score\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fd537f06-96fd-4319-91e9-dc4f28657147",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.transforms import v2    \n",
    "def compute_normalization_stats(h5_path, classes=None):\n",
    "    \"\"\"\n",
    "    Compute mean and standard deviation for z-score normalization.\n",
    "    \n",
    "    Args:\n",
    "        h5_path (str): Path to HDF5 file\n",
    "        classes (list, optional): List of classes to include in computation\n",
    "        \n",
    "    Returns:\n",
    "        tuple: (mean, std) computed across all data points\n",
    "    \"\"\"\n",
    "    with h5py.File(h5_path, 'r') as h5_file:\n",
    "        data = h5_file['data'][:]\n",
    "        labels = h5_file['labels'][:]\n",
    "        \n",
    "        if classes is not None:\n",
    "            # Filter data for selected classes\n",
    "            mask = np.isin(labels, classes)\n",
    "            data = data[mask]\n",
    "        \n",
    "        # Compute statistics across all dimensions\n",
    "        mean = np.mean(data)\n",
    "        std = np.std(data)\n",
    "        \n",
    "        print(f\"Computed statistics: mean = {mean:.4f}, std = {std:.4f}\")\n",
    "        \n",
    "        return mean, std\n",
    "        \n",
    "class ParticleDataset(Dataset):\n",
    "    \"\"\"Custom Dataset for particle data with flexible class selection and normalization.\"\"\"\n",
    "    def __init__(self, h5_path, classes=[0, 1], transform=None, mean=None, std=None,padding=False,indicies=list(range(0, 10000))):\n",
    "        self.h5_file = h5py.File(h5_path, 'r')\n",
    "        data = self.h5_file['data'][:]\n",
    "        labels = self.h5_file['labels'][:]\n",
    "        self.padding = padding\n",
    "        # Filter data for selected classes\n",
    "        mask = np.isin(labels, classes)\n",
    "        if indicies is None:\n",
    "            self.data = data[mask][:]\n",
    "            self.labels = labels[mask][:] \n",
    "        else:\n",
    "            self.data = data[mask][indicies]\n",
    "            self.labels = labels[mask][indicies]\n",
    "        \n",
    "        # Create class mapping to handle non-consecutive class indices\n",
    "        self.class_to_idx = {c: i for i, c in enumerate(classes)}\n",
    "        self.num_classes = len(classes)\n",
    "        \n",
    "        # Map original labels to new consecutive indices\n",
    "        self.labels = np.array([self.class_to_idx[label] for label in self.labels])\n",
    "        self.transform = transform\n",
    "        self.mean = mean\n",
    "        self.std = std\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Get particle data\n",
    "        particle = self.data[idx]  # Shape: (16, 201)\n",
    "        \n",
    "        # Apply normalization if mean and std are provided\n",
    "        if self.mean is not None and self.std is not None:\n",
    "            particle = (particle - self.mean) / self.std\n",
    "        \n",
    "        # Convert to torch tensor for better interpolation\n",
    "        particle_tensor = torch.FloatTensor(particle).unsqueeze(0)  # Add channel dim\n",
    "        \n",
    "        # Resize to (16, 16) using bicubic interpolation\n",
    "        resized = torch.nn.functional.interpolate(\n",
    "            particle_tensor.unsqueeze(0),  # Add batch dim\n",
    "            size=(16, 201),\n",
    "            mode='bicubic',\n",
    "            align_corners=True\n",
    "        ).squeeze(0).squeeze(0)  # Remove batch and channel dims\n",
    "        \n",
    "        final_tensor = resized.unsqueeze(0).repeat(3, 1, 1)  # Repeat across 3 channels\n",
    "        \n",
    "        if self.transform:\n",
    "            final_tensor = self.transform(final_tensor)\n",
    "        \n",
    "        # Create one-hot encoded label\n",
    "        label_idx = self.labels[idx]\n",
    "        label_onehot = torch.zeros(self.num_classes)\n",
    "        label_onehot[label_idx] = 1\n",
    "        \n",
    "        return final_tensor, label_onehot\n",
    "\n",
    "    def close(self):\n",
    "        self.h5_file.close()     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "eb2cddf3-50b0-451e-8d94-3c50397f6a4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "\n",
    "def distance_matrix(a, b):\n",
    "    a_expanded = a.view(-1, 1)\n",
    "    b_expanded = b.view(1, -1)\n",
    "\n",
    "    return torch.abs(a_expanded - b_expanded)\n",
    "\n",
    "def knn_divergence(points_x, points_y, k, smoothing_kernel=None):\n",
    "    xx_distances = distance_matrix(points_x, points_x)\n",
    "    xy_distances = distance_matrix(points_x, points_y) # one row for every sample in x, one col for every sample in y\n",
    "    # print(xx_distances.shape, xy_distances.shape)\n",
    "    # if the sets have different sizes\n",
    "    # e.g. y has twice as many points -> the distance to the 3rd closest point in x should be the same as the distance to the 6th point in y\n",
    "    k_multiplier = points_y.shape[0] / points_x.shape[0]\n",
    "\n",
    "    k_dist_xx = torch.sort(xx_distances, dim=1)[0][:, k]\n",
    "    k_dist_xy = torch.sort(xy_distances, dim=1)[0][:, (k * k_multiplier).to(torch.int)]\n",
    "\n",
    "    # optional: smoothen the distances \n",
    "    # (so that it matters less whether a point is the i-th or the (i+1)-th closest neighbor)\n",
    "    if smoothing_kernel != None:\n",
    "            # torch conv1d demands a channel dimension, hence the (un)squeezing\n",
    "            k_dist_xx = torch.nn.functional.conv1d(k_dist_xx.unsqueeze(1), weight=smoothing_kernel.view(1, 1, -1)).flatten(1)\n",
    "            k_dist_xy = torch.nn.functional.conv1d(k_dist_xy.unsqueeze(1), weight=smoothing_kernel.view(1, 1, -1)).flatten(1)\n",
    "\n",
    "    # return torch.mean((1 - k_dist_xx / k_dist_xy)**2)\n",
    "    return torch.mean((k_dist_xx - k_dist_xy)**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8a576945-297a-457d-9f63-32b9c04ac973",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def compute_contrast(imgs: torch.Tensor, dim=(1,2,3)):\n",
    "    return imgs.std(dim=dim)\n",
    "    \n",
    "# def compute_contrast(imgs: torch.Tensor, dim=(1, 2, 3)):\n",
    "#     \"\"\"\n",
    "#     Compute contrast values for a batch of images.\n",
    "#     Args:\n",
    "#         imgs (torch.Tensor): Batch of images of shape (B, C, H, W).\n",
    "#         dim (tuple): Dimensions over which to compute the contrast.\n",
    "#     Returns:\n",
    "#         torch.Tensor: Contrast value for each image in the batch (B,).\n",
    "#     \"\"\"\n",
    "#     return (imgs.amax(dim=dim) - imgs.amin(dim=dim)) / imgs.mean(dim=dim)\n",
    "\n",
    "def monotonicity_loss(predictions: torch.Tensor, images: torch.Tensor):\n",
    "    \"\"\"\n",
    "    Enforces that predictions follow the same monotonic order as contrast.\n",
    "    If c1 < c2 then y1 < y2 should hold.\n",
    "    Args:\n",
    "        predictions (torch.Tensor): Predicted values (B,).\n",
    "        images (torch.Tensor): Batch of images (B, C, H, W) to compute contrast.\n",
    "    Returns:\n",
    "        torch.Tensor: Scalar monotonicity loss.\n",
    "    \"\"\"\n",
    "    # Compute contrast values\n",
    "    contrast_values = compute_contrast(images)  # Shape: (B,)\n",
    "    \n",
    "    # Ensure predictions are the right shape\n",
    "    predictions = predictions.squeeze()\n",
    "    \n",
    "    # Compute pairwise differences for both contrast and predictions\n",
    "    contrast_diff = contrast_values.unsqueeze(0) - contrast_values.unsqueeze(1)  # (B, B)\n",
    "    pred_diff = predictions.unsqueeze(0) - predictions.unsqueeze(1)  # (B, B)\n",
    "    \n",
    "    # Find where contrast_i < contrast_j\n",
    "    contrast_pairs = (contrast_diff < 0)\n",
    "    \n",
    "    # When contrast_i < contrast_j, we want pred_i < pred_j\n",
    "    # So we penalize cases where pred_i >= pred_j\n",
    "    violations = torch.relu(pred_diff) * contrast_pairs\n",
    "    \n",
    "    # Sum up all violations and normalize by number of valid pairs\n",
    "    num_pairs = contrast_pairs.sum()\n",
    "    if num_pairs > 0:\n",
    "        loss = violations.sum() / num_pairs\n",
    "    else:\n",
    "        loss = torch.tensor(0.0, device=predictions.device)\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "46ced2fa-7cb7-41fb-bac6-6f267e46debb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchsummary import summary\n",
    "# resnet = torch.hub.load('pytorch/vision:v0.10.0', 'resnet18', pretrained=False)\n",
    "# # resnet .fc = nn.Linear(resnet.fc.in_features, 1)\n",
    "# resnet .fc = nn.Sequential(\n",
    "#     nn.Linear(resnet.fc.in_features, 32),\n",
    "#     nn.ReLU(),\n",
    "#     nn.Linear(32, 1))\n",
    "# summary(resnet, input_size=(3, 16, 201),device='cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "93d414e4-aee4-42ef-a922-7bbda00008f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda:11\n",
      "Computed statistics: mean = 7407.4357, std = 1323.8027\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(42)\n",
    "DEVICE = \"cuda:11\"\n",
    "# Device configuration\n",
    "device = torch.device(DEVICE if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "# classes = [0]\n",
    "transform = v2.Compose([\n",
    "    v2.RandomVerticalFlip(p=0.5),\n",
    "    v2.RandomHorizontalFlip(p=0.5),\n",
    "])\n",
    "mean, std = compute_normalization_stats('dataset/brightfield_particles.hdf5', classes=[0])\n",
    "dataset_80 = ParticleDataset(h5_path='dataset/brightfield_particles.hdf5',\n",
    "                          classes=[0],\n",
    "                          mean=mean,\n",
    "                          std=std,\n",
    "                          padding=True,\n",
    "                             transform = transform,\n",
    "                             indicies=list(range(0, 20000)),\n",
    "                         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "53312d91-ce1f-48ed-b639-22384bdd3d87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def compute_contrast(img):\n",
    "#     return (img.max()-img.min())/img.mean()\n",
    "# img = dataset_80[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f944d584-0830-44af-a01b-a390e3f4a120",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "def generate_label_distribution(num_points=10000, mean=76, std=22.5, min_value=10, max_value=None):\n",
    "    \"\"\"\n",
    "    Generate a tensor of points sampled from a normal distribution with specified mean and standard deviation\n",
    "    while rejecting points outside the optional min and max value constraints.\n",
    "    \n",
    "    Args:\n",
    "        num_points (int): Number of points to generate\n",
    "        mean (float): Mean of the distribution\n",
    "        std (float): Standard deviation of the distribution\n",
    "        min_value (float, optional): Minimum value of the distribution (inclusive)\n",
    "        max_value (float, optional): Maximum value of the distribution (inclusive)\n",
    "    \n",
    "    Returns:\n",
    "        torch.Tensor: Tensor of generated points within the specified range\n",
    "    \"\"\"\n",
    "    points = torch.empty(0)  # Initialize an empty tensor to store valid points\n",
    "\n",
    "    while points.numel() < num_points:\n",
    "        # Generate points from normal distribution\n",
    "        generated_points = torch.normal(mean=mean, std=std, size=(num_points,))\n",
    "        \n",
    "        # Filter points based on the min and max values\n",
    "        if min_value is not None:\n",
    "            generated_points = generated_points[generated_points >= min_value]\n",
    "        if max_value is not None:\n",
    "            generated_points = generated_points[generated_points <= max_value]\n",
    "        \n",
    "        # Add the valid points to the tensor\n",
    "        points = torch.cat((points, generated_points))\n",
    "    # Return only the first `num_points` points\n",
    "    return points[:num_points]\n",
    "    \n",
    "batch_size = 10000\n",
    "num_points = 10000\n",
    "dataloader_80 = DataLoader(dataset_80, batch_size=batch_size, shuffle=True)\n",
    "label_points_80 = generate_label_distribution(num_points, mean=76, std=22.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "91e9fc85-2b2f-4665-a1aa-b0d9c9a19d38",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_resnet(model, dataloaders, label_points, device, num_epochs=10, learning_rate=3e-2):\n",
    "    \"\"\"\n",
    "    Train ResNet model using KNN divergence loss with early stopping and learning rate scheduling.\n",
    "    \n",
    "    Args:\n",
    "        model (torch.nn.Module): ResNet model\n",
    "        dataloader (torch.utils.data.DataLoader): Training dataloader\n",
    "        label_points (torch.Tensor): Pre-generated label points\n",
    "        device (torch.device): Device to train on\n",
    "        num_epochs (int): Number of training epochs\n",
    "        learning_rate (float): Learning rate for optimizer\n",
    "    \n",
    "    Returns:\n",
    "        model: Trained model\n",
    "        best_loss: Best training loss achieved\n",
    "    \"\"\"\n",
    "    # Move label points to the specified device\n",
    "    labels = [label.to(device, non_blocking=True) for label in label_points] \n",
    "    # labels_80, labels_300 = labels[0],labels[1] \n",
    "    # Prepare k values for KNN divergence\n",
    "    ks = [torch.arange(2, label_point.shape[0]//10, dtype=torch.int) for label_point in label_points]    \n",
    "    # Setup optimizer\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    \n",
    "    # Setup learning rate scheduler with patience of 8\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        optimizer, mode='min', patience=8, factor=0.5\n",
    "    )\n",
    "    \n",
    "    model.to(device)\n",
    "   \n",
    "    # Early stopping parameters\n",
    "    best_loss = float('inf')\n",
    "    patience = 20\n",
    "    patience_counter = 0\n",
    "    best_model_state = None\n",
    "    \n",
    "    # Training loop\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        batch_count = 0\n",
    "        total_m_loss = 0\n",
    "        for idx, (label, dataloader) in enumerate(zip(labels, dataloaders)):\n",
    "                for batch_images, _ in dataloader:\n",
    "                    batch_count += 1\n",
    "                    gt = torch.clone(label)\n",
    "                    batch_images = batch_images.to(device)\n",
    "                    \n",
    "                    # Zero gradients\n",
    "                    optimizer.zero_grad()\n",
    "                    \n",
    "                    # Forward pass: generate predictions\n",
    "                    batch_predictions = model(batch_images)\n",
    "                    \n",
    "                    # Compute KNN divergence loss\n",
    "                    loss = knn_divergence(batch_predictions, gt, ks[idx])\n",
    "                    monotony_loss = 0.2 * monotonicity_loss(batch_predictions,batch_images)\n",
    "                    loss+=monotony_loss\n",
    "                    # Backward pass and optimize\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "                    total_m_loss+=monotony_loss\n",
    "                    total_loss += loss.item()\n",
    "        \n",
    "        # Calculate average loss for the epoch\n",
    "        avg_loss = total_loss / batch_count\n",
    "        avg_total_m_loss = total_m_loss / batch_count\n",
    "        current_lr = optimizer.param_groups[0]['lr']\n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {avg_loss:.4f}, M_loss: {avg_total_m_loss:.4f}, LR: {current_lr:.2e}')\n",
    "        \n",
    "        # Learning rate scheduling\n",
    "        scheduler.step(avg_loss)\n",
    "        \n",
    "        # Early stopping check\n",
    "        if avg_loss < best_loss:\n",
    "            best_loss = avg_loss\n",
    "            patience_counter = 0\n",
    "            best_model_state = model.state_dict().copy()\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= patience:\n",
    "                print(f'Early stopping triggered after {epoch + 1} epochs')\n",
    "                # Restore best model\n",
    "                model.load_state_dict(best_model_state)\n",
    "                break\n",
    "    \n",
    "    return model, best_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9b31a67-7dae-46ea-8df3-9e59d59a8d7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /root/.cache/torch/hub/pytorch_vision_v0.10.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/300], Loss: 1073.6974, M_loss: 1.0908, LR: 3.00e-02\n",
      "Epoch [2/300], Loss: 217.9788, M_loss: 0.6461, LR: 3.00e-02\n",
      "Epoch [3/300], Loss: 3.1284, M_loss: 0.5762, LR: 3.00e-02\n",
      "Epoch [4/300], Loss: 38.2732, M_loss: 1.5252, LR: 3.00e-02\n",
      "Epoch [5/300], Loss: 24.6062, M_loss: 2.3672, LR: 3.00e-02\n",
      "Epoch [6/300], Loss: 32.7017, M_loss: 1.3844, LR: 3.00e-02\n",
      "Epoch [7/300], Loss: 6.0639, M_loss: 1.1656, LR: 3.00e-02\n",
      "Epoch [8/300], Loss: 2.2528, M_loss: 1.0979, LR: 3.00e-02\n",
      "Epoch [9/300], Loss: 3.7996, M_loss: 1.0339, LR: 3.00e-02\n",
      "Epoch [10/300], Loss: 3.6650, M_loss: 0.6033, LR: 3.00e-02\n",
      "Epoch [11/300], Loss: 2.8305, M_loss: 0.5729, LR: 3.00e-02\n",
      "Epoch [12/300], Loss: 2.2992, M_loss: 0.7885, LR: 3.00e-02\n",
      "Epoch [13/300], Loss: 1.9984, M_loss: 0.9727, LR: 3.00e-02\n",
      "Epoch [14/300], Loss: 2.2128, M_loss: 1.0849, LR: 3.00e-02\n",
      "Epoch [15/300], Loss: 2.4794, M_loss: 1.1438, LR: 3.00e-02\n",
      "Epoch [16/300], Loss: 2.5010, M_loss: 1.1674, LR: 3.00e-02\n",
      "Epoch [17/300], Loss: 2.2270, M_loss: 1.1767, LR: 3.00e-02\n",
      "Epoch [18/300], Loss: 1.9569, M_loss: 1.1760, LR: 3.00e-02\n",
      "Epoch [19/300], Loss: 1.8705, M_loss: 1.1709, LR: 3.00e-02\n",
      "Epoch [20/300], Loss: 1.8697, M_loss: 1.1576, LR: 3.00e-02\n",
      "Epoch [21/300], Loss: 1.9218, M_loss: 1.1400, LR: 3.00e-02\n",
      "Epoch [22/300], Loss: 1.9107, M_loss: 1.1232, LR: 3.00e-02\n",
      "Epoch [23/300], Loss: 1.8467, M_loss: 1.0945, LR: 3.00e-02\n",
      "Epoch [24/300], Loss: 1.8051, M_loss: 1.0701, LR: 3.00e-02\n",
      "Epoch [25/300], Loss: 1.7913, M_loss: 1.0376, LR: 3.00e-02\n",
      "Epoch [26/300], Loss: 1.7933, M_loss: 1.0106, LR: 3.00e-02\n",
      "Epoch [27/300], Loss: 1.7764, M_loss: 0.9886, LR: 3.00e-02\n",
      "Epoch [28/300], Loss: 1.7654, M_loss: 0.9758, LR: 3.00e-02\n",
      "Epoch [29/300], Loss: 1.7528, M_loss: 0.9703, LR: 3.00e-02\n",
      "Epoch [30/300], Loss: 1.7395, M_loss: 0.9685, LR: 3.00e-02\n",
      "Epoch [31/300], Loss: 1.7314, M_loss: 0.9684, LR: 3.00e-02\n",
      "Epoch [32/300], Loss: 1.7266, M_loss: 0.9751, LR: 3.00e-02\n",
      "Epoch [33/300], Loss: 1.7189, M_loss: 0.9880, LR: 3.00e-02\n",
      "Epoch [34/300], Loss: 1.7096, M_loss: 0.9966, LR: 3.00e-02\n",
      "Epoch [35/300], Loss: 1.7072, M_loss: 1.0009, LR: 3.00e-02\n",
      "Epoch [36/300], Loss: 1.7056, M_loss: 0.9913, LR: 3.00e-02\n",
      "Epoch [37/300], Loss: 1.6947, M_loss: 0.9781, LR: 3.00e-02\n",
      "Epoch [38/300], Loss: 1.6843, M_loss: 0.9619, LR: 3.00e-02\n",
      "Epoch [39/300], Loss: 1.6769, M_loss: 0.9466, LR: 3.00e-02\n",
      "Epoch [40/300], Loss: 1.6737, M_loss: 0.9325, LR: 3.00e-02\n",
      "Epoch [41/300], Loss: 1.6690, M_loss: 0.9236, LR: 3.00e-02\n",
      "Epoch [42/300], Loss: 1.6621, M_loss: 0.9172, LR: 3.00e-02\n",
      "Epoch [43/300], Loss: 1.6510, M_loss: 0.9137, LR: 3.00e-02\n",
      "Epoch [44/300], Loss: 1.6425, M_loss: 0.9108, LR: 3.00e-02\n",
      "Epoch [45/300], Loss: 1.6385, M_loss: 0.9085, LR: 3.00e-02\n",
      "Epoch [46/300], Loss: 1.6272, M_loss: 0.9057, LR: 3.00e-02\n",
      "Epoch [47/300], Loss: 1.6211, M_loss: 0.9048, LR: 3.00e-02\n",
      "Epoch [48/300], Loss: 1.6136, M_loss: 0.9063, LR: 3.00e-02\n",
      "Epoch [49/300], Loss: 1.6073, M_loss: 0.9029, LR: 3.00e-02\n",
      "Epoch [50/300], Loss: 1.5993, M_loss: 0.9030, LR: 3.00e-02\n",
      "Epoch [51/300], Loss: 1.5917, M_loss: 0.9039, LR: 3.00e-02\n",
      "Epoch [52/300], Loss: 1.5815, M_loss: 0.9064, LR: 3.00e-02\n",
      "Epoch [53/300], Loss: 1.5762, M_loss: 0.9089, LR: 3.00e-02\n",
      "Epoch [54/300], Loss: 1.5682, M_loss: 0.9252, LR: 3.00e-02\n",
      "Epoch [55/300], Loss: 1.5678, M_loss: 0.9352, LR: 3.00e-02\n",
      "Epoch [56/300], Loss: 1.5647, M_loss: 0.9412, LR: 3.00e-02\n",
      "Epoch [57/300], Loss: 1.5623, M_loss: 0.9560, LR: 3.00e-02\n",
      "Epoch [58/300], Loss: 1.5614, M_loss: 0.9706, LR: 3.00e-02\n",
      "Epoch [59/300], Loss: 1.5596, M_loss: 0.9801, LR: 3.00e-02\n",
      "Epoch [60/300], Loss: 1.5530, M_loss: 0.9820, LR: 3.00e-02\n",
      "Epoch [61/300], Loss: 1.5512, M_loss: 0.9884, LR: 3.00e-02\n",
      "Epoch [62/300], Loss: 1.5474, M_loss: 0.9965, LR: 3.00e-02\n",
      "Epoch [63/300], Loss: 1.5449, M_loss: 0.9915, LR: 3.00e-02\n",
      "Epoch [64/300], Loss: 1.5462, M_loss: 0.9899, LR: 3.00e-02\n",
      "Epoch [65/300], Loss: 1.5436, M_loss: 0.9886, LR: 3.00e-02\n",
      "Epoch [66/300], Loss: 1.5424, M_loss: 0.9878, LR: 3.00e-02\n",
      "Epoch [67/300], Loss: 1.5413, M_loss: 0.9892, LR: 3.00e-02\n",
      "Epoch [68/300], Loss: 1.5375, M_loss: 0.9907, LR: 3.00e-02\n",
      "Epoch [69/300], Loss: 1.5406, M_loss: 0.9926, LR: 3.00e-02\n",
      "Epoch [70/300], Loss: 1.5380, M_loss: 0.9879, LR: 3.00e-02\n",
      "Epoch [71/300], Loss: 1.5362, M_loss: 0.9840, LR: 3.00e-02\n",
      "Epoch [72/300], Loss: 1.5356, M_loss: 0.9878, LR: 3.00e-02\n",
      "Epoch [73/300], Loss: 1.5340, M_loss: 0.9845, LR: 3.00e-02\n",
      "Epoch [74/300], Loss: 1.5337, M_loss: 0.9823, LR: 3.00e-02\n",
      "Epoch [75/300], Loss: 1.5306, M_loss: 0.9879, LR: 3.00e-02\n",
      "Epoch [76/300], Loss: 1.5306, M_loss: 0.9912, LR: 3.00e-02\n",
      "Epoch [77/300], Loss: 1.5289, M_loss: 0.9859, LR: 3.00e-02\n",
      "Epoch [78/300], Loss: 1.5285, M_loss: 0.9875, LR: 3.00e-02\n",
      "Epoch [79/300], Loss: 1.5269, M_loss: 0.9897, LR: 3.00e-02\n",
      "Epoch [80/300], Loss: 1.5266, M_loss: 0.9873, LR: 3.00e-02\n",
      "Epoch [81/300], Loss: 1.5238, M_loss: 0.9867, LR: 3.00e-02\n",
      "Epoch [82/300], Loss: 1.5247, M_loss: 0.9898, LR: 3.00e-02\n",
      "Epoch [83/300], Loss: 1.5263, M_loss: 0.9812, LR: 3.00e-02\n",
      "Epoch [84/300], Loss: 1.5242, M_loss: 0.9851, LR: 3.00e-02\n",
      "Epoch [85/300], Loss: 1.5215, M_loss: 0.9895, LR: 3.00e-02\n",
      "Epoch [86/300], Loss: 1.5209, M_loss: 0.9839, LR: 3.00e-02\n",
      "Epoch [87/300], Loss: 1.5205, M_loss: 0.9850, LR: 3.00e-02\n",
      "Epoch [88/300], Loss: 1.5201, M_loss: 0.9863, LR: 3.00e-02\n",
      "Epoch [89/300], Loss: 1.5180, M_loss: 0.9835, LR: 3.00e-02\n",
      "Epoch [90/300], Loss: 1.5189, M_loss: 0.9886, LR: 3.00e-02\n",
      "Epoch [91/300], Loss: 1.5171, M_loss: 0.9869, LR: 3.00e-02\n",
      "Epoch [92/300], Loss: 1.5161, M_loss: 0.9860, LR: 3.00e-02\n",
      "Epoch [93/300], Loss: 1.5172, M_loss: 0.9892, LR: 3.00e-02\n",
      "Epoch [94/300], Loss: 1.5129, M_loss: 0.9872, LR: 3.00e-02\n",
      "Epoch [95/300], Loss: 1.5158, M_loss: 0.9900, LR: 3.00e-02\n",
      "Epoch [96/300], Loss: 1.5139, M_loss: 0.9926, LR: 3.00e-02\n",
      "Epoch [97/300], Loss: 1.5151, M_loss: 0.9896, LR: 3.00e-02\n",
      "Epoch [98/300], Loss: 1.5130, M_loss: 0.9867, LR: 3.00e-02\n",
      "Epoch [99/300], Loss: 1.5142, M_loss: 0.9898, LR: 3.00e-02\n",
      "Epoch [100/300], Loss: 1.5152, M_loss: 0.9882, LR: 3.00e-02\n",
      "Epoch [101/300], Loss: 1.5113, M_loss: 0.9923, LR: 3.00e-02\n",
      "Epoch [102/300], Loss: 1.5124, M_loss: 0.9917, LR: 3.00e-02\n",
      "Epoch [103/300], Loss: 1.5108, M_loss: 0.9847, LR: 3.00e-02\n",
      "Epoch [104/300], Loss: 1.5109, M_loss: 0.9889, LR: 3.00e-02\n",
      "Epoch [105/300], Loss: 1.5104, M_loss: 0.9888, LR: 3.00e-02\n",
      "Epoch [106/300], Loss: 1.5089, M_loss: 0.9845, LR: 3.00e-02\n",
      "Epoch [107/300], Loss: 1.5098, M_loss: 0.9905, LR: 3.00e-02\n",
      "Epoch [108/300], Loss: 1.5083, M_loss: 0.9884, LR: 3.00e-02\n",
      "Epoch [109/300], Loss: 1.5059, M_loss: 0.9865, LR: 3.00e-02\n",
      "Epoch [110/300], Loss: 1.5076, M_loss: 0.9850, LR: 3.00e-02\n",
      "Epoch [111/300], Loss: 1.5127, M_loss: 0.9850, LR: 3.00e-02\n",
      "Epoch [112/300], Loss: 1.5118, M_loss: 0.9853, LR: 3.00e-02\n",
      "Epoch [113/300], Loss: 1.5062, M_loss: 0.9873, LR: 3.00e-02\n",
      "Epoch [114/300], Loss: 1.5061, M_loss: 0.9826, LR: 3.00e-02\n",
      "Epoch [115/300], Loss: 1.5069, M_loss: 0.9837, LR: 3.00e-02\n",
      "Epoch [116/300], Loss: 1.5033, M_loss: 0.9829, LR: 3.00e-02\n",
      "Epoch [117/300], Loss: 1.5037, M_loss: 0.9858, LR: 3.00e-02\n",
      "Epoch [118/300], Loss: 1.5057, M_loss: 0.9874, LR: 3.00e-02\n",
      "Epoch [119/300], Loss: 1.5060, M_loss: 0.9793, LR: 3.00e-02\n",
      "Epoch [120/300], Loss: 1.5099, M_loss: 0.9828, LR: 3.00e-02\n",
      "Epoch [121/300], Loss: 1.5042, M_loss: 0.9783, LR: 3.00e-02\n",
      "Epoch [122/300], Loss: 1.5020, M_loss: 0.9750, LR: 3.00e-02\n",
      "Epoch [123/300], Loss: 1.5052, M_loss: 0.9870, LR: 3.00e-02\n",
      "Epoch [124/300], Loss: 1.5033, M_loss: 0.9811, LR: 3.00e-02\n",
      "Epoch [125/300], Loss: 1.5048, M_loss: 0.9863, LR: 3.00e-02\n",
      "Epoch [126/300], Loss: 1.5051, M_loss: 0.9792, LR: 3.00e-02\n",
      "Epoch [127/300], Loss: 1.5007, M_loss: 0.9747, LR: 3.00e-02\n",
      "Epoch [128/300], Loss: 1.4993, M_loss: 0.9802, LR: 3.00e-02\n",
      "Epoch [129/300], Loss: 1.5054, M_loss: 0.9713, LR: 3.00e-02\n",
      "Epoch [130/300], Loss: 1.4997, M_loss: 0.9819, LR: 3.00e-02\n",
      "Epoch [131/300], Loss: 1.4978, M_loss: 0.9804, LR: 3.00e-02\n",
      "Epoch [132/300], Loss: 1.4991, M_loss: 0.9741, LR: 3.00e-02\n",
      "Epoch [133/300], Loss: 1.5008, M_loss: 0.9844, LR: 3.00e-02\n",
      "Epoch [134/300], Loss: 1.5004, M_loss: 0.9745, LR: 3.00e-02\n",
      "Epoch [135/300], Loss: 1.5002, M_loss: 0.9757, LR: 3.00e-02\n",
      "Epoch [136/300], Loss: 1.4997, M_loss: 0.9755, LR: 3.00e-02\n",
      "Epoch [137/300], Loss: 1.4971, M_loss: 0.9676, LR: 3.00e-02\n",
      "Epoch [138/300], Loss: 1.4948, M_loss: 0.9727, LR: 3.00e-02\n",
      "Epoch [139/300], Loss: 1.4931, M_loss: 0.9711, LR: 3.00e-02\n",
      "Epoch [140/300], Loss: 1.4938, M_loss: 0.9737, LR: 3.00e-02\n",
      "Epoch [141/300], Loss: 1.4932, M_loss: 0.9714, LR: 3.00e-02\n",
      "Epoch [142/300], Loss: 1.4946, M_loss: 0.9744, LR: 3.00e-02\n",
      "Epoch [143/300], Loss: 1.4916, M_loss: 0.9672, LR: 3.00e-02\n",
      "Epoch [144/300], Loss: 1.4894, M_loss: 0.9694, LR: 3.00e-02\n",
      "Epoch [145/300], Loss: 1.4928, M_loss: 0.9724, LR: 3.00e-02\n",
      "Epoch [146/300], Loss: 1.4903, M_loss: 0.9674, LR: 3.00e-02\n",
      "Epoch [147/300], Loss: 1.4882, M_loss: 0.9727, LR: 3.00e-02\n",
      "Epoch [148/300], Loss: 1.4883, M_loss: 0.9684, LR: 3.00e-02\n",
      "Epoch [149/300], Loss: 1.4865, M_loss: 0.9640, LR: 3.00e-02\n",
      "Epoch [150/300], Loss: 1.4867, M_loss: 0.9716, LR: 3.00e-02\n",
      "Epoch [151/300], Loss: 1.4883, M_loss: 0.9656, LR: 3.00e-02\n",
      "Epoch [152/300], Loss: 1.4845, M_loss: 0.9597, LR: 3.00e-02\n",
      "Epoch [153/300], Loss: 1.4877, M_loss: 0.9653, LR: 3.00e-02\n",
      "Epoch [154/300], Loss: 1.4850, M_loss: 0.9589, LR: 3.00e-02\n",
      "Epoch [155/300], Loss: 1.4839, M_loss: 0.9688, LR: 3.00e-02\n",
      "Epoch [156/300], Loss: 1.4829, M_loss: 0.9654, LR: 3.00e-02\n",
      "Epoch [157/300], Loss: 1.4842, M_loss: 0.9618, LR: 3.00e-02\n",
      "Epoch [158/300], Loss: 1.4856, M_loss: 0.9723, LR: 3.00e-02\n",
      "Epoch [159/300], Loss: 1.4824, M_loss: 0.9659, LR: 3.00e-02\n",
      "Epoch [160/300], Loss: 1.4822, M_loss: 0.9682, LR: 3.00e-02\n",
      "Epoch [161/300], Loss: 1.4807, M_loss: 0.9680, LR: 3.00e-02\n",
      "Epoch [162/300], Loss: 1.4872, M_loss: 0.9623, LR: 3.00e-02\n",
      "Epoch [163/300], Loss: 1.4818, M_loss: 0.9705, LR: 3.00e-02\n",
      "Epoch [164/300], Loss: 1.4795, M_loss: 0.9584, LR: 3.00e-02\n",
      "Epoch [165/300], Loss: 1.4799, M_loss: 0.9616, LR: 3.00e-02\n",
      "Epoch [166/300], Loss: 1.4803, M_loss: 0.9671, LR: 3.00e-02\n",
      "Epoch [167/300], Loss: 1.4769, M_loss: 0.9661, LR: 3.00e-02\n",
      "Epoch [168/300], Loss: 1.4774, M_loss: 0.9761, LR: 3.00e-02\n",
      "Epoch [169/300], Loss: 1.4763, M_loss: 0.9706, LR: 3.00e-02\n",
      "Epoch [170/300], Loss: 1.4771, M_loss: 0.9676, LR: 3.00e-02\n",
      "Epoch [171/300], Loss: 1.4735, M_loss: 0.9634, LR: 3.00e-02\n",
      "Epoch [172/300], Loss: 1.4735, M_loss: 0.9556, LR: 3.00e-02\n",
      "Epoch [173/300], Loss: 1.4700, M_loss: 0.9597, LR: 3.00e-02\n",
      "Epoch [174/300], Loss: 1.4739, M_loss: 0.9641, LR: 3.00e-02\n",
      "Epoch [175/300], Loss: 1.4717, M_loss: 0.9626, LR: 3.00e-02\n",
      "Epoch [176/300], Loss: 1.4754, M_loss: 0.9711, LR: 3.00e-02\n",
      "Epoch [177/300], Loss: 1.4685, M_loss: 0.9545, LR: 3.00e-02\n",
      "Epoch [178/300], Loss: 1.4704, M_loss: 0.9627, LR: 3.00e-02\n",
      "Epoch [179/300], Loss: 1.4678, M_loss: 0.9571, LR: 3.00e-02\n",
      "Epoch [180/300], Loss: 1.4684, M_loss: 0.9584, LR: 3.00e-02\n",
      "Epoch [181/300], Loss: 1.4691, M_loss: 0.9566, LR: 3.00e-02\n",
      "Epoch [182/300], Loss: 1.4726, M_loss: 0.9541, LR: 3.00e-02\n",
      "Epoch [183/300], Loss: 1.4721, M_loss: 0.9671, LR: 3.00e-02\n",
      "Epoch [184/300], Loss: 1.4686, M_loss: 0.9524, LR: 3.00e-02\n",
      "Epoch [185/300], Loss: 1.4622, M_loss: 0.9515, LR: 3.00e-02\n",
      "Epoch [186/300], Loss: 1.4655, M_loss: 0.9545, LR: 3.00e-02\n",
      "Epoch [187/300], Loss: 1.4571, M_loss: 0.9574, LR: 3.00e-02\n",
      "Epoch [188/300], Loss: 1.4630, M_loss: 0.9628, LR: 3.00e-02\n",
      "Epoch [189/300], Loss: 1.4645, M_loss: 0.9511, LR: 3.00e-02\n",
      "Epoch [190/300], Loss: 1.4599, M_loss: 0.9388, LR: 3.00e-02\n",
      "Epoch [191/300], Loss: 1.4625, M_loss: 0.9509, LR: 3.00e-02\n",
      "Epoch [192/300], Loss: 1.4572, M_loss: 0.9459, LR: 3.00e-02\n",
      "Epoch [193/300], Loss: 1.4524, M_loss: 0.9426, LR: 3.00e-02\n",
      "Epoch [194/300], Loss: 1.4510, M_loss: 0.9393, LR: 3.00e-02\n",
      "Epoch [195/300], Loss: 1.4493, M_loss: 0.9386, LR: 3.00e-02\n",
      "Epoch [196/300], Loss: 1.4498, M_loss: 0.9442, LR: 3.00e-02\n",
      "Epoch [197/300], Loss: 1.4471, M_loss: 0.9420, LR: 3.00e-02\n",
      "Epoch [198/300], Loss: 1.4469, M_loss: 0.9404, LR: 3.00e-02\n",
      "Epoch [199/300], Loss: 1.4488, M_loss: 0.9388, LR: 3.00e-02\n",
      "Epoch [200/300], Loss: 1.4449, M_loss: 0.9412, LR: 3.00e-02\n",
      "Epoch [201/300], Loss: 1.4443, M_loss: 0.9340, LR: 3.00e-02\n",
      "Epoch [202/300], Loss: 1.4512, M_loss: 0.9402, LR: 3.00e-02\n",
      "Epoch [203/300], Loss: 1.4494, M_loss: 0.9210, LR: 3.00e-02\n",
      "Epoch [204/300], Loss: 1.4422, M_loss: 0.9459, LR: 3.00e-02\n",
      "Epoch [205/300], Loss: 1.4415, M_loss: 0.9407, LR: 3.00e-02\n",
      "Epoch [206/300], Loss: 1.4365, M_loss: 0.9468, LR: 3.00e-02\n",
      "Epoch [207/300], Loss: 1.4381, M_loss: 0.9302, LR: 3.00e-02\n",
      "Epoch [208/300], Loss: 1.4336, M_loss: 0.9361, LR: 3.00e-02\n",
      "Epoch [209/300], Loss: 1.4330, M_loss: 0.9438, LR: 3.00e-02\n"
     ]
    }
   ],
   "source": [
    "resnet = torch.hub.load('pytorch/vision:v0.10.0', 'resnet18', weights=None)\n",
    "dataloaders = (dataloader_80,)\n",
    "label_points = (label_points_80,)\n",
    "# resnet .fc = nn.Linear(resnet.fc.in_features, 1)\n",
    "resnet .fc = nn.Sequential(\n",
    "    nn.Linear(resnet.fc.in_features, 128),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(128, 64),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(64, 1)\n",
    ")\n",
    "resnet,best_loss = train_resnet(resnet, dataloaders, label_points, device, num_epochs=300)\n",
    "resnet.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb50403a-d1fb-48db-8f94-bd4ecdc2cb39",
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet.eval()\n",
    "resnet(dataset_80[0][0].unsqueeze(0).to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95f6ed8f-4812-4f11-bf7f-dfacc6aa106b",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    a= next(iter(dataloader_80))[0]\n",
    "    out  = resnet(a.to(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feee442d-c61a-45b2-babd-1d9639491032",
   "metadata": {},
   "outputs": [],
   "source": [
    "out.min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb0433e3-4879-4299-803b-9b70aa230b7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot histograms\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.hist(label_points_80 , bins=50, alpha=0.6, label='Ground Truth', color='blue', density=True)\n",
    "plt.hist(out.cpu().detach().numpy(), bins=50, alpha=0.6, label='Prediction', color='red', density=True)\n",
    "\n",
    "# Labels and legend\n",
    "plt.xlabel('Diamerter[nm]')\n",
    "plt.ylabel('Density[norm.]')\n",
    "plt.title('Ground Truth vs Prediction Distribution_80nm')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "# Show plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f0ec387-8e2c-49ab-af56-44bd3b8fe702",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_dataset_300 = ParticleDataset(h5_path='dataset/brightfield_particles.hdf5',\n",
    "                          classes=[1],\n",
    "                          mean=mean,\n",
    "                          std=std,\n",
    "                          padding=True,\n",
    "                        transform = None,\n",
    "                        indicies = None\n",
    "                         )\n",
    "plot_dataloader_300 = DataLoader(plot_dataset_300 , batch_size=len(plot_dataset_300))\n",
    "with torch.no_grad():\n",
    "    out_2 = next(iter(plot_dataloader_300))[0]\n",
    "    out_2  = resnet(out_2.to(device)).cpu().detach().numpy()\n",
    "label_points_300 = generate_label_distribution(len(plot_dataset_300), mean=302, std=25)\n",
    "print(out_2.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c532407c-9edd-45b3-bdc7-6ce915615c45",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_contrast(imgs:torch.Tensor,dim=(1, 2, 3)):\n",
    "    \"\"\"\n",
    "        args:\n",
    "            imgs:batch_images\n",
    "            dim: dim\n",
    "        return\n",
    "            output: contrast value of each image in the batch\n",
    "    \"\"\"\n",
    "    return (imgs.amax(dim=dim)-img.amax(dim=dim))/img.mean(dim=dim)\n",
    "out\n",
    "with torch.no_grad():\n",
    "    a = next(iter(dataloader_80))[0]\n",
    "    c = compute_contrast(a)\n",
    "    sorted_indices = torch.argsort(c) \n",
    "    a=a[sorted_indices]\n",
    "    out = resnet(a.to(device))\n",
    "plt.scatter(list(range(0,len(out[0:1000]))),out[0:1000].cpu())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a883cd88-2497-40e1-9b8d-910f5ab1d665",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "# Example list of values\n",
    "# values =out_2+(302-out_2.mean())\n",
    "values =out_2\n",
    "# values = o\n",
    "# Set up the plot\n",
    "plt.figure(figsize=(8, 6))\n",
    "\n",
    "# Plot the histogram\n",
    "plt.hist(values, bins=50, color='red',label='Prediction',alpha=0.7, density=True)\n",
    "plt.hist(label_points_300  , bins=50, alpha=0.6, label='Ground Truth', color='blue', density=True)\n",
    "# Add labels\n",
    "# Labels and legend\n",
    "plt.xlabel('Diamerter[nm]')\n",
    "plt.ylabel('Density[norm.]')\n",
    "plt.title('Ground Truth vs Prediction Distribution_300nm')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "# Show plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "721d3d88-0e70-4e1e-aad6-bf5685e7333c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_dataset_1300 = ParticleDataset(h5_path='dataset/brightfield_particles.hdf5',\n",
    "                          classes=[2],\n",
    "                          mean=mean,\n",
    "                          std=std,\n",
    "                          padding=True,\n",
    "                        transform = None,\n",
    "                        indicies=None\n",
    "                         )\n",
    "plot_dataloader_1300 = DataLoader(plot_dataset_1300, batch_size=len(plot_dataset_1300))\n",
    "with torch.no_grad():\n",
    "    out_3 = next(iter(plot_dataloader_1300))[0]\n",
    "    out_3  = resnet(out_3.to(device)).cpu().detach().numpy()\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Example list of values\n",
    "values = out_3\n",
    "# values = o\n",
    "# Set up the plot\n",
    "plt.figure(figsize=(8, 6))\n",
    "\n",
    "# Plot the histogram\n",
    "plt.hist(values, bins=100, density=True, color='blue', alpha=0.7)\n",
    "\n",
    "# Add labels\n",
    "plt.title('Distribution of Values_1300nm')\n",
    "plt.xlabel('Value')\n",
    "plt.ylabel('Density')\n",
    "\n",
    "# Show plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97fb1c84-8237-4268-8cd7-be87607073d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_dataset_80 = ParticleDataset(h5_path='dataset/brightfield_particles.hdf5',\n",
    "                          classes=[0],\n",
    "                          mean=mean,\n",
    "                          std=std,\n",
    "                          padding=True,\n",
    "                            transform = None,indicies=list(range(0, 20001))\n",
    "                         )\n",
    "plot_dataloader_80 = DataLoader(plot_dataset_80, batch_size=20000)\n",
    "with torch.no_grad():\n",
    "    out_4 = next(iter(plot_dataloader_80))[0]\n",
    "    out_4  = resnet(out_4.to(device)).cpu().detach().numpy()\n",
    "import matplotlib.pyplot as plt\n",
    "# Example list of values\n",
    "values = out_4\n",
    "# values = o\n",
    "# Set up the plot\n",
    "plt.figure(figsize=(8, 6))\n",
    "\n",
    "# Plot the histogram\n",
    "plt.hist(values, bins=100, density=True, color='blue', alpha=0.7)\n",
    "\n",
    "# Add labels\n",
    "plt.title('Distribution of Values_80nm')\n",
    "plt.xlabel('Value')\n",
    "plt.ylabel('Density')\n",
    "\n",
    "# Show plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "230d8064-0bdd-4a18-9fec-61148331e950",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    output_map = resnet.conv1(dataset_80[0][0].unsqueeze(0).to(device)).squeeze(0).cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97cc3170-d622-4221-8449-cf2f6a915198",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(18, 10))\n",
    "f=torch.clone(output_map[3])\n",
    "# f[f<(f.mean())]=0\n",
    "ax.imshow(f,cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aa5d029-fd07-40ff-9bd5-66a626d800d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    imgs = next(iter(dataloader_80))[0]  # (10000,3,16,201)\n",
    "    img = imgs[2000].unsqueeze(0)  # (1,3,16,201)\n",
    "    \n",
    "    # Get prediction for original image\n",
    "    size_1 = resnet(img.to(device)).cpu()\n",
    "    size_1 = size_1.squeeze(0)\n",
    "    \n",
    "    # Flip the image horizontally (along the last dimension)\n",
    "    img_flipped = torch.flip(img, dims=[-1])\n",
    "    \n",
    "    # Get prediction for flipped image\n",
    "    size_2 = resnet(img_flipped.to(device)).cpu()\n",
    "    size_2 = size_2.squeeze(0)\n",
    "    \n",
    "    # Print both predictions\n",
    "    print(f\"Original image size prediction: {size_1.item():.3f}\")\n",
    "    print(f\"Flipped image size prediction: {size_2.item():.3f}\")\n",
    "    print(f\"Absolute difference: {abs(size_1.item() - size_2.item()):.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f220a167-dfa9-4632-8562-3b899edca031",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plot_dataset_80 = ParticleDataset(h5_path='dataset/brightfield_particles.hdf5',\n",
    "                          classes=[0],\n",
    "                          mean=mean,\n",
    "                          std=std,\n",
    "                          padding=True,\n",
    "                            transform = None\n",
    "                         )\n",
    "plot_dataloader = DataLoader(plot_dataset_80, batch_size=batch_size)\n",
    "with torch.no_grad():\n",
    "    imgs = next(iter(plot_dataloader))[0]  # (10000,3,16,201)\n",
    "    sizes = resnet(imgs.to(device)).cpu() \n",
    "    \n",
    "max_size, max_idx = sizes.max(dim=0)\n",
    "min_size, min_idx = sizes.min(dim=0)\n",
    "\n",
    "# Compute middle size (median)\n",
    "mid_size = sizes.median()\n",
    "mid_idx = (sizes - mid_size).abs().argmin()\n",
    "mid_idx = torch.tensor([mid_idx], dtype=torch.int64)\n",
    "\n",
    "# Create 9 intermediate values between min, mid, and max\n",
    "intermediate_sizes, intermediate_indices = [], []\n",
    "for fraction in torch.linspace(0, 1, steps=9):\n",
    "    interp_size = min_size + fraction * (max_size - min_size)\n",
    "    closest_idx = (sizes - interp_size).abs().argmin()\n",
    "    intermediate_sizes.append(sizes[closest_idx])\n",
    "    intermediate_indices.append(closest_idx)\n",
    "\n",
    "# Convert indices to tensor\n",
    "intermediate_indices = torch.tensor(intermediate_indices, dtype=torch.int64)\n",
    "\n",
    "# Get images and resize them\n",
    "resized_images = [\n",
    "    torch.nn.functional.interpolate(\n",
    "        imgs[idx][0:1].unsqueeze(0), size=(16, 32), mode=\"bilinear\", align_corners=False\n",
    "    ).squeeze(0)[0]\n",
    "    for idx in intermediate_indices\n",
    "]\n",
    "\n",
    "# Create 3x3 subplot\n",
    "fig, axes = plt.subplots(3, 3, figsize=(12, 9))\n",
    "\n",
    "# Plot images\n",
    "for ax, img, size in zip(axes.flat, resized_images, intermediate_sizes):\n",
    "    ax.imshow(img, cmap=\"gray\")\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "    ax.title.set_text(f\"size: {size.item()}\")\n",
    "\n",
    "# Adjust layout and show\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "iscat",
   "language": "python",
   "name": "iscat"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
