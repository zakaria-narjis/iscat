{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current Directory: d:\\workspace\\iscat\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.chdir(\"..\")\n",
    "print(\"Current Directory:\", os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.data_processing.dataset import iScatDataset\n",
    "from src.data_processing.utils import Utils\n",
    "import torch\n",
    "DEVICE= 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "data_path = os.path.join('dataset', '2024_11_11', 'Metasurface', 'Chip_02')\n",
    "image_paths,target_paths = Utils.get_data_paths(data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading TIFF images to Memory: 100%|██████████| 4/4 [00:00<00:00, 36.96it/s]\n",
      "Loading TIFF images to Memory: 100%|██████████| 1/1 [00:00<00:00, 41.78it/s]\n"
     ]
    }
   ],
   "source": [
    "image_size=256\n",
    "fluo_masks_indices=[0]\n",
    "seg_method = \"kmeans\"\n",
    "train_dataset = iScatDataset(image_paths[:-1], target_paths[:-1], preload_image=True,image_size = (image_size,image_size),apply_augmentation=True,normalize=False,device=DEVICE,fluo_masks_indices=fluo_masks_indices,seg_method=seg_method)\n",
    "valid_dataset = iScatDataset([image_paths[-1]],[target_paths[-1]],preload_image=True,image_size = (image_size,image_size),apply_augmentation=False,normalize=False,device=DEVICE,fluo_masks_indices=fluo_masks_indices,seg_method=seg_method)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, Dataset\n",
    "def create_dataloaders(train_dataset, test_dataset, batch_size=4):\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "    return train_loader, val_loader\n",
    "train_loader, val_loader = create_dataloaders(train_dataset, valid_dataset, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from monai.losses import DiceLoss\n",
    "from monai.metrics import MeanIoU\n",
    "import numpy as np\n",
    "from monai.networks.utils import one_hot\n",
    "\n",
    "def z_score_normalize(images: torch.Tensor, eps: float = 1e-8) -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Normalize a batch of images using z-score normalization.\n",
    "\n",
    "    Args:\n",
    "        images (torch.Tensor): Input tensor of shape (N, 3, H, W), where\n",
    "                               N is the batch size, 3 is the number of channels,\n",
    "                               H and W are height and width.\n",
    "        eps (float): A small value to avoid division by zero (default: 1e-8).\n",
    "\n",
    "    Returns:\n",
    "        torch.Tensor: Z-score normalized tensor of the same shape as `images`.\n",
    "    \"\"\"\n",
    "    normalized_images = images/65535.0\n",
    "    # Compute mean and std for each image in the batch\n",
    "    mean = normalized_images .mean(dim=(1, 2, 3), keepdim=True)  # Shape: (N, 1, 1, 1)\n",
    "    std = normalized_images .std(dim=(1, 2, 3), keepdim=True)    # Shape: (N, 1, 1, 1)\n",
    "    \n",
    "    # Perform z-score normalization\n",
    "    normalized_images = (images - mean) / (std + eps)\n",
    "    return normalized_images\n",
    "\n",
    "class MultiClassUNet(nn.Module):\n",
    "    def __init__(self, in_channels=3, num_classes=1, init_features=32):\n",
    "        super(MultiClassUNet, self).__init__()\n",
    "        \n",
    "        # Load the pretrained model and modify the final layer\n",
    "        model = torch.hub.load('mateuszbuda/brain-segmentation-pytorch', 'unet', \n",
    "                               in_channels=in_channels, \n",
    "                               out_channels=1, \n",
    "                               init_features=init_features, \n",
    "                               pretrained=True)\n",
    "        \n",
    "        # Replace the final convolution layer to match number of classes\n",
    "        model.conv = nn.Conv2d(init_features, num_classes, kernel_size=1)\n",
    "        \n",
    "        self.model = model\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from monai.losses import DiceLoss\n",
    "from monai.metrics import MeanIoU\n",
    "import numpy as np\n",
    "from enum import Enum\n",
    "from typing import Dict, Any\n",
    "\n",
    "class LossType(Enum):\n",
    "    CROSSENTROPY = \"crossentropy\"\n",
    "    DICE = \"dice\"\n",
    "    COMBINED = \"combined\"\n",
    "\n",
    "class UNetTrainer:\n",
    "    def __init__(\n",
    "        self,\n",
    "        model: nn.Module,\n",
    "        device: torch.device,\n",
    "        loss_type: LossType,\n",
    "        learning_rate: float = 1e-4,\n",
    "        log_dir: str = \"runs/unet_training\"\n",
    "    ):\n",
    "        self.model = model.to(device)\n",
    "        self.device = device\n",
    "        self.loss_type = loss_type\n",
    "        \n",
    "        # Initialize loss functions\n",
    "        self.ce_loss = nn.BCEWithLogitsLoss()\n",
    "        self.dice_loss = DiceLoss(\n",
    "            sigmoid=True,\n",
    "            smooth_nr=1.0,\n",
    "            smooth_dr=1.0,\n",
    "            squared_pred=False,  # Changed to False for potentially better stability\n",
    "            batch=True,\n",
    "            reduction=\"mean\"\n",
    "        )\n",
    "        \n",
    "        # Initialize metrics\n",
    "        self.miou_metric = MeanIoU(include_background=True, reduction=\"mean\")\n",
    "        \n",
    "        self.optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "        \n",
    "        # Add learning rate scheduler\n",
    "        self.scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "            self.optimizer,\n",
    "            mode='min',\n",
    "            factor=0.5,\n",
    "            patience=5,\n",
    "            verbose=True\n",
    "        )\n",
    "        self.writer = SummaryWriter(log_dir)\n",
    "\n",
    "    def compute_loss(self, predictions: torch.Tensor, targets: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Compute the specified loss\n",
    "        predictions: (batch_size, 1, height, width)\n",
    "        targets: (batch_size, height, width)\n",
    "        \"\"\"\n",
    "        # Add channel dimension to targets if needed\n",
    "        if len(targets.shape) == 3:\n",
    "            targets = targets.unsqueeze(1)\n",
    "            \n",
    "        # Ensure targets are float\n",
    "        targets = targets.float()\n",
    "        \n",
    "        if self.loss_type == LossType.CROSSENTROPY:\n",
    "            return self.ce_loss(predictions, targets)\n",
    "        elif self.loss_type == LossType.DICE:\n",
    "            return self.dice_loss(predictions, targets)\n",
    "        else:  # COMBINED\n",
    "            ce = self.ce_loss(predictions, targets)\n",
    "            dice = self.dice_loss(predictions, targets)\n",
    "            return ce + dice\n",
    "\n",
    "    def compute_metrics(self, predictions: torch.Tensor, targets: torch.Tensor) -> float:\n",
    "        \"\"\"\n",
    "        Compute mean IoU metric\n",
    "        predictions: (batch_size, 1, height, width)\n",
    "        targets: (batch_size, height, width)\n",
    "        \"\"\"\n",
    "        # Ensure predictions are binary\n",
    "        pred_masks = (torch.sigmoid(predictions) > 0.5).float()\n",
    "        \n",
    "        # Add channel dimension to targets if needed\n",
    "        if len(targets.shape) == 3:\n",
    "            targets = targets.unsqueeze(1)\n",
    "        \n",
    "        # Convert predictions and targets to one-hot format (required by MeanIoU)\n",
    "        # Shape: (batch_size, 2, height, width)\n",
    "        pred_one_hot = torch.cat([1 - pred_masks, pred_masks], dim=1)\n",
    "        target_one_hot = torch.cat([1 - targets, targets], dim=1)\n",
    "        \n",
    "        # Compute IoU\n",
    "        metric = self.miou_metric(pred_one_hot, target_one_hot)\n",
    "        \n",
    "        # Return mean IoU (average across classes)\n",
    "        return metric.mean().item()\n",
    "\n",
    "    def train_epoch(self, train_loader, epoch: int) -> Dict[str, float]:\n",
    "        self.model.train()\n",
    "        total_loss = 0.0\n",
    "        total_miou = 0.0\n",
    "        batches = 0\n",
    "\n",
    "        for batch_idx, (images, masks) in enumerate(train_loader):\n",
    "            # Move data to device and normalize images\n",
    "            images = z_score_normalize(images).to(self.device)\n",
    "            masks = masks.to(self.device)\n",
    "            \n",
    "            # Forward pass\n",
    "            self.optimizer.zero_grad()\n",
    "            predictions = self.model(images)  # Shape: (batch_size, 1, height, width)\n",
    "            \n",
    "            # Compute loss and backpropagate\n",
    "            loss = self.compute_loss(predictions, masks)\n",
    "            loss.backward()\n",
    "            \n",
    "            # Add gradient clipping\n",
    "            torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)\n",
    "            \n",
    "            self.optimizer.step()\n",
    "            \n",
    "            # Compute metrics\n",
    "            miou = self.compute_metrics(predictions, masks)\n",
    "            \n",
    "            # Update running statistics\n",
    "            total_loss += loss.item()\n",
    "            total_miou += miou\n",
    "            batches += 1\n",
    "            \n",
    "            # Log to TensorBoard\n",
    "            step = epoch * len(train_loader) + batch_idx\n",
    "            self.writer.add_scalar('Train/Loss', loss.item(), step)\n",
    "            self.writer.add_scalar('Train/mIoU', miou, step)\n",
    "            \n",
    "            # Print batch progress\n",
    "            if batch_idx % 10 == 0:\n",
    "                print(f'Batch {batch_idx}/{len(train_loader)}, Loss: {loss.item():.4f}, mIoU: {miou:.4f}')\n",
    "\n",
    "        return {\n",
    "            'loss': total_loss / batches,\n",
    "            'miou': total_miou / batches\n",
    "        }\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def validate(self, val_loader) -> Dict[str, float]:\n",
    "        self.model.eval()\n",
    "        total_loss = 0.0\n",
    "        total_miou = 0.0\n",
    "        batches = 0\n",
    "\n",
    "        for images, masks in val_loader:\n",
    "            images = z_score_normalize(images).to(self.device)\n",
    "            masks = masks.to(self.device)\n",
    "            \n",
    "            predictions = self.model(images)\n",
    "            loss = self.compute_loss(predictions, masks)\n",
    "            miou = self.compute_metrics(predictions, masks)\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            total_miou += miou\n",
    "            batches += 1\n",
    "\n",
    "        return {\n",
    "            'val_loss': total_loss / batches,\n",
    "            'val_miou': total_miou / batches\n",
    "        }\n",
    "\n",
    "    def train(self, train_loader, val_loader, num_epochs: int):\n",
    "            best_val_miou = 0.0\n",
    "            patience = 10  # Early stopping patience\n",
    "            no_improve = 0\n",
    "            \n",
    "            for epoch in range(num_epochs):\n",
    "                # Training phase\n",
    "                train_metrics = self.train_epoch(train_loader, epoch)\n",
    "                \n",
    "                # Validation phase\n",
    "                val_metrics = self.validate(val_loader)\n",
    "                \n",
    "                # Update learning rate scheduler\n",
    "                self.scheduler.step(val_metrics['val_loss'])\n",
    "                \n",
    "                # Log validation metrics\n",
    "                self.writer.add_scalar('Validation/Loss', val_metrics['val_loss'], epoch)\n",
    "                self.writer.add_scalar('Validation/mIoU', val_metrics['val_miou'], epoch)\n",
    "                \n",
    "                # Save best model\n",
    "                if val_metrics['val_miou'] > best_val_miou:\n",
    "                    best_val_miou = val_metrics['val_miou']\n",
    "                    torch.save({\n",
    "                        'epoch': epoch,\n",
    "                        'model_state_dict': self.model.state_dict(),\n",
    "                        'optimizer_state_dict': self.optimizer.state_dict(),\n",
    "                        'val_miou': val_metrics['val_miou'],\n",
    "                    }, 'best_model.pth')\n",
    "                    no_improve = 0\n",
    "                else:\n",
    "                    no_improve += 1\n",
    "                \n",
    "                # Early stopping\n",
    "                if no_improve >= patience:\n",
    "                    print(f'Early stopping triggered after {patience} epochs without improvement')\n",
    "                    break\n",
    "                \n",
    "                print(f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "                print(f\"Train Loss: {train_metrics['loss']:.4f}, Train mIoU: {train_metrics['miou']:.4f}\")\n",
    "                print(f\"Val Loss: {val_metrics['val_loss']:.4f}, Val mIoU: {val_metrics['val_miou']:.4f}\")\n",
    "                print(f\"Learning rate: {self.optimizer.param_groups[0]['lr']:.2e}\")\n",
    "                print(\"-\" * 50)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\zakar/.cache\\torch\\hub\\mateuszbuda_brain-segmentation-pytorch_master\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0/13, Loss: 0.9900, mIoU: 0.0025\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Initialize model\n",
    "model = MultiClassUNet(in_channels=3, num_classes=1, init_features=32)\n",
    "\n",
    "# Initialize trainer\n",
    "trainer = UNetTrainer(\n",
    "    model=model,\n",
    "    device=device,\n",
    "    loss_type=LossType.DICE,  # or CROSSENTROPY or DICE\n",
    "    learning_rate=1e-4\n",
    ")\n",
    "\n",
    "\n",
    "trainer.train(train_loader, val_loader, num_epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(4.2531, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "a= train_dataset[1][0]\n",
    "print(z_score_normalize(a.unsqueeze(0)).max())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "iscat",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
